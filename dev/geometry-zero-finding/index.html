<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Visualizing a step for various zero-finding algorithms · Roots</title><meta name="title" content="Visualizing a step for various zero-finding algorithms · Roots"/><meta property="og:title" content="Visualizing a step for various zero-finding algorithms · Roots"/><meta property="twitter:title" content="Visualizing a step for various zero-finding algorithms · Roots"/><meta name="description" content="Documentation for Roots."/><meta property="og:description" content="Documentation for Roots."/><meta property="twitter:description" content="Documentation for Roots."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="Roots logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">Roots</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Roots.jl</a></li><li class="is-active"><a class="tocitem" href>Visualizing a step for various zero-finding algorithms</a><ul class="internal"><li><a class="tocitem" href="#Newton&#39;s-method"><span>Newton&#39;s method</span></a></li><li><a class="tocitem" href="#Secant-method"><span>Secant method</span></a></li><li><a class="tocitem" href="#Inverse-quadratic-and-cubic-methods"><span>Inverse quadratic and cubic methods</span></a></li><li><a class="tocitem" href="#Higher-derivative-variations-on-Newton&#39;s-method"><span>Higher derivative variations on Newton&#39;s method</span></a></li></ul></li><li><a class="tocitem" href="../reference/">Reference/API</a></li><li><a class="tocitem" href="../roots/">An overview of <code>Roots</code></a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Visualizing a step for various zero-finding algorithms</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Visualizing a step for various zero-finding algorithms</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaMath/Roots.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaMath/Roots.jl/blob/master/docs/src/geometry-zero-finding.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Visualizing-a-step-for-various-zero-finding-algorithms"><a class="docs-heading-anchor" href="#Visualizing-a-step-for-various-zero-finding-algorithms">Visualizing a step for various zero-finding algorithms</a><a id="Visualizing-a-step-for-various-zero-finding-algorithms-1"></a><a class="docs-heading-anchor-permalink" href="#Visualizing-a-step-for-various-zero-finding-algorithms" title="Permalink"></a></h1><p>We illustrate the geometry behind a single step of several different, non-bracketing, zero-finding algorithms, beginning with, perhaps, the most famous, Newton&#39;s method.</p><h2 id="Newton&#39;s-method"><a class="docs-heading-anchor" href="#Newton&#39;s-method">Newton&#39;s method</a><a id="Newton&#39;s-method-1"></a><a class="docs-heading-anchor-permalink" href="#Newton&#39;s-method" title="Permalink"></a></h2><p>In addition to <code>Roots</code>, we use the <code>Plots</code> and <code>ForwardDiff</code> packages:</p><pre><code class="language-julia hljs">using Roots
using Plots, ForwardDiff
Base.adjoint(f::Function)  = x  -&gt; ForwardDiff.derivative(f, float(x)) # f&#39; will compute derivative</code></pre><p>A zero-finding algorithm solves <span>$f(x) = 0$</span> or possibly <span>$f(x,p) = 0$</span> for a value of <span>$x$</span>. Here we discuss iterative algorithms which take one <em>or more</em> past steps to produce the next step. (That is <span>$x_{n+1} = F(x_n, x_{n-1}, ..., x_1, x_0)$</span>, for some <span>$F$</span> representing the algorithm).</p><p><a href="https://en.wikipedia.org/wiki/Newton%27s_method">Newton&#39;s Method</a> is a zero-finding <em>iterative algorithm</em> easily introduced in an introductory calculus class once the concept of a <em>tangent line</em> is presented.</p><p>The value <span>$x_{n+1}$</span> is described as the <em>intersection point</em> of the <span>$x$</span>-axis with the tangent line through <span>$(x_n, f(x_n))$</span>. To be explicit, we substitute <span>$(x_{n+1},0)$</span> into the tangent line equation <span>$y = f(x_n) + f&#39;(x_n)\cdot(x-x_n)$</span>:</p><p class="math-container">\[0 = f(x_n) + f&#39;(x_n) \cdot (x_{n+1} - x_n).\]</p><p>Solving gives the update formula:</p><p class="math-container">\[x_{n+1} = x_n - \frac{f(x_n)}{f&#39;(x_n)}.\]</p><p>The convergence is not guaranteed for all initial guesses, <span>$x_0$</span>, but for a <em>simple zero</em> of a continuously differentiable function <span>$f(x)$</span> there is <strong>some</strong> interval about the zero, <span>$\alpha$</span>, such that <em>quadratic convergence</em> will happen.</p><p>The geometry of Newton&#39;s method can be illustrated by graphing the tangent line.</p><p>The function <span>$f(x) = x^5 - x - 1$</span> does not have a readily available closed-form solution for its lone real zero, it being a fifth-degree polynomial. However, a graph, or other means, can show the function has one zero between <span>$1$</span> and <span>$2$</span>, closer to <span>$1$</span>. Starting with <span>$x_0=1.4$</span>, we get a visual of <span>$x_1$</span> as follows:</p><pre><code class="language-julia hljs">f(x) = x^5 - x - 1
x0 = 1.4
α = find_zero((f, f&#39;), x0, Roots.Newton())

tl(x) = f(x0) + f&#39;(x0)*(x-x0)
x1 = x0 - f(x0)/f&#39;(x0)

p = plot(f, 1.1, 1.5; legend=false, linewidth=3)
plot!(zero)

plot!(tl; color=&quot;red&quot;, linewidth=3)

scatter!([x0, x1], [0, 0]; markercolor=&quot;blue&quot;)
annotate!([(x0,0,&quot;x0&quot;, :bottom), (x1, 0, &quot;x1&quot;, :bottom)])

scatter!([x0], [f(x0)]; markercolor=:blue)

scatter!([α], [0]; markercolor=:blue)
annotate!([(α, 0, &quot;α&quot;, :top)])
p</code></pre><img src="c136fa37.svg" alt="Example block output"/><p>We used <code>Roots.Newton()</code> to identify the zero.</p><h2 id="Secant-method"><a class="docs-heading-anchor" href="#Secant-method">Secant method</a><a id="Secant-method-1"></a><a class="docs-heading-anchor-permalink" href="#Secant-method" title="Permalink"></a></h2><p>The secant method is much older than Newton&#39;s method, though similar in that the intersection of a line with the <span>$x$</span>-axis is used as the next step in the algorithm. The slope of the secant line is (historically) easy to compute, unlike the slope of the tangent line which requires the notion of a derivative. The secant method begins with <em>two</em> initial points, <span>$x_0$</span> and <span>$x_1$</span> and uses the secant line instead of the tangent line. The secant line has slope <span>$(f(x_1)-f(x_0))/(x_1-x_0)$</span>. This yields the algorithm:</p><p class="math-container">\[x_{n+1} = x_n - \left(\frac{f(x_n)-f(x_{n-1})}{x_n-x_{n-1}}\right)^{-1} \cdot f(x_n).\]</p><p>We can visualize the secant method easily enough. Suppose we start with <span>$x_0=1.4$</span> and <span>$x_1=1.3$</span>:</p><pre><code class="language-julia hljs">x0, x1 = 1.4, 1.3
x2 = x1 - (x1-x0)/(f(x1)-f(x0)) * f(x1)
sl(x) = f(x1) + (f(x1)-f(x0))/(x1-x0) * (x-x1)

p = plot(f, 1.1, 1.5; legend=false, linewidth=3)
plot!(zero)

plot!(sl, color=:red, linewidth=3)

scatter!([x0, x1, x2], [0,0,0]; markercolor=:blue)
annotate!([(x0,0,&quot;x0&quot;, :bottom), (x1, 0, &quot;x1&quot;, :bottom), (x2,0,&quot;x2&quot;, :bottom)])
scatter!([x0, x1], [f(x0), f(x1)]; markercolor=:blue)

scatter!([α],[0]; markercolor=:blue)
annotate!([(α, 0, &quot;α&quot;, :top)])

p</code></pre><img src="d5961ffd.svg" alt="Example block output"/><p>The secant method is implemented in <code>Secant()</code>.</p><p>Steffensen&#39;s method (<code>Root.Steffensen()</code>) is related to the secant method, though the points are not <span>$x_n$</span> and <span>$x_{n-1}$</span>, rather  <span>$x_n + f(x_n)$</span> and <span>$x_n$</span>. As <span>$x_n$</span> gets close to <span>$\alpha$</span>, <span>$f(x_n)$</span> gets close to <span>$0$</span>, so this method converges at an asymptotic rate like Newton&#39;s method. (Though with a tradeoff, as the secant method needs only one new function evaluation per step, Steffensen&#39;s require two.)</p><h3 id="An-inverse-view"><a class="docs-heading-anchor" href="#An-inverse-view">An inverse view</a><a id="An-inverse-view-1"></a><a class="docs-heading-anchor-permalink" href="#An-inverse-view" title="Permalink"></a></h3><p>The secant line is a natural object as presented above, but can be viewed a bit differently. Consider the two points <span>$(f(x_0), x_0)$</span> and <span>$(f(x_1),x_1)$</span>. Two non-equal points determine a line. In this case, we have inverted the <span>$x$</span> and <span>$y$</span> values, so we invert the coordinates of the line. To find <span>$x = my + b$</span>, or some other form of the line involves solving two equations with two unknowns. Each equation comes by using the known point:</p><p class="math-container">\[\begin{align*}
x_0 &amp;= m \cdot f(x_0) + b\\
x_1 &amp;= m \cdot f(x_1) + b
\end{align*}\]</p><p>This <em>linear</em> set of equations can be solved, some <code>SymPy</code> code would look like:</p><pre><code class="nohighlight hljs">using SymPy
@syms x0, y0, x1, y1, m ,b
u = solve([x0 ~ y0 * m + b, x1 ~ y1 * m + b], (m,b))</code></pre><p>Producing</p><pre><code class="nohighlight hljs">Dict{Any, Any} with 2 entries:
  b =&gt; (-x0*y1 + x1*y0)/(y0 - y1)
  m =&gt; (x0 - x1)/(y0 - y1)</code></pre><p>The value of <code>m</code> is the reciprocal of the slope, as we have inverted the perspective. The value of <code>b</code> is where the inverse line intersects the <span>$x$</span> axis and is the same as the secant method algorithm:</p><pre><code class="nohighlight hljs">sm = x1 - y1 * (x1-x0)/(y1-y0)
simplify(sm - u[b])</code></pre><p>leading to:</p><pre><code class="nohighlight hljs">0</code></pre><h2 id="Inverse-quadratic-and-cubic-methods"><a class="docs-heading-anchor" href="#Inverse-quadratic-and-cubic-methods">Inverse quadratic and cubic methods</a><a id="Inverse-quadratic-and-cubic-methods-1"></a><a class="docs-heading-anchor-permalink" href="#Inverse-quadratic-and-cubic-methods" title="Permalink"></a></h2><p>Brent&#39;s method (<code>Roots.Brent()</code>) is a bracketing method which utilizes an inverse quadratic step to speed up convergence beyond the secant method. The inverse quadratic step uses the fact that three (non-collinear) points determine a quadratic polynomial. As above, this is done with the inverse of the points, <span>$(x_{n-2}, f(x_{n-2}))$</span>, <span>$(x_{n-1}, f(x_{n-1}))$</span>, and <span>$(x_{n}, f(x_{n}))$</span>. Using the same method illustrated above, it can be shown that, with <span>$\Delta_{i,j} = f(x_i) - f(x_j)$</span>:</p><p class="math-container">\[x_{n+1} = \frac{x_{n-2}f(x_{n-1})f(x_n)}{\Delta_{i-2, i-1}\Delta_{i-2,i}}
+ \frac{f(x_{n-2})x_{n-1}f(x_n)}{\Delta_{n-1, n-2}\Delta_{n-1,n}}
+ \frac{f(x_{n-2})f(x_{n-1})x_n}{\Delta_{n,n-2}\Delta_{n,n-1}}.\]</p><pre><code class="nohighlight hljs">x0, x1, x2 = xs = 1.4, 1.3, 1.2
fx0, fx1, fx2 = ys = f.(xs)

@syms x[0:2], y[0:2], a, b, c
u = solve([xᵢ ~ a*yᵢ^2 + b * yᵢ + c for (xᵢ, yᵢ) ∈ zip(x, y)], (a, b, c))
x3 = u[c]
for (k, v) ∈ u
  for (xᵢ, yᵢ, x,y) ∈ zip(x, y, xs, ys)
    v = v(xᵢ =&gt; x, yᵢ =&gt; y)
  end
  u[k] = v
end
u[a], u[b], u[c]</code></pre><p>Which returns</p><pre><code class="nohighlight hljs">(-0.00930682152998560, 0.104752944517765, 1.17057129242798)</code></pre><p>This last value, <code>c</code>, is also computed in the <code>(3,0)</code> method of the <code>LithBoonkkampIJzerman</code> algorithm, which implements this method:</p><pre><code class="language-julia hljs">x0, x1, x2 = 1.4, 1.3, 1.2
xs = [x0, x1,x2]
x3 = Roots.lmm(Roots.LithBoonkkampIJzerman{3,0}(), xs, f.(xs))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1.1705712924279839</code></pre><p>With this, we can visualize:</p><pre><code class="language-julia hljs">a, b, c = (-0.00930682152998560, 0.104752944517765, 1.17057129242798)
iq(y) = a * y^2 + b * y + c

p = plot(f, 1.1, 1.5; legend=false, linewidth=3)
plot!(zero)

ys′ = range(f(1.1), f(1.5), length=100)
plot!(iq.(ys′), ys′; color=:red, linewidth=3)

scatter!(xs, f.(xs);  markercolor=:blue)
scatter!([x3], [f(x3)]; markercolor=:blue)
annotate!([(x0,0,&quot;x0&quot;, :bottom), (x1, 0, &quot;x1&quot;, :bottom),
	(x2,0,&quot;x2&quot;, :bottom),  (x3,0,&quot;x3&quot;, :bottom)])
scatter!(xs, zero.(xs);  markercolor=:blue)

scatter!([α],[0]; markercolor=:blue)
annotate!([(α, 0, &quot;α&quot;, :top)])

p</code></pre><img src="2b596742.svg" alt="Example block output"/><p>Inverse cubic is similar to the above, though we have <span>$4$</span> past points used to determine the next one. In this example, we solve the resulting linear system of equations numerically (the <code>Roots.LithBoonkkampIJzerman{4,0}()</code> method implements the algorithm):</p><pre><code class="language-julia hljs">xs = [1.4, 1.35, 1.3, 1.25]
ys = f.(xs)
A = zeros(Float64, 4, 4)
for i ∈ reverse(0:3)
    A[:,4-i] .= ys.^i
end
a, b, c, d = A \ xs
ic(y) = a * y^3 + b * y^2 + c * y + d

p = plot(f, 1.1, 1.5; legend=false, linewidth=3)
plot!(zero)
plot!(ic.(ys′), ys′;  color=:red, linewidth=3)

x4 = d
scatter!(vcat(xs, x4), zeros(5); markercolor=:blue)
for (i,x) ∈ enumerate(xs)
    annotate!([(x, 0, &quot;x$(i-1)&quot;, :bottom)])
end
annotate!([(x4, 0, &quot;x4&quot;, :bottom)])
scatter!(xs, f.(xs); markercolor=:blue)

scatter!([α],[0]; markercolor=:blue)
annotate!([(α, 0, &quot;α&quot;, :top)])

p</code></pre><img src="7abfc77a.svg" alt="Example block output"/><p>We can see graphically that for this function and the chosen values, the inverse quadratic and inverse cubic fits are very close to the actual zero, suggesting a rapid convergence. <code>Roots.Brent()</code>, <code>Roots.Chandrapatlu()</code>, and <code>Roots.AlefeldPotraShi()</code> are different bracketing methods which use an inverse quadratic step <em>when</em> the algorithm deems it productive, falling back to other estimates when not. Similarly, the inverse cubic step is utilized by <code>Roots.A42()</code>, as possible. The <code>LithBoonkkampIJzerman{S,0}</code> methods use <span>$S$</span> previous points (<span>$S \geq 2$</span>) and the corresponding inverse polynomial step to progress. Since these are not bracketed, the algorithms are only guaranteed to converge for nearby initial guesses.</p><h2 id="Higher-derivative-variations-on-Newton&#39;s-method"><a class="docs-heading-anchor" href="#Higher-derivative-variations-on-Newton&#39;s-method">Higher derivative variations on Newton&#39;s method</a><a id="Higher-derivative-variations-on-Newton&#39;s-method-1"></a><a class="docs-heading-anchor-permalink" href="#Higher-derivative-variations-on-Newton&#39;s-method" title="Permalink"></a></h2><p>We can visualize Newton&#39;s method differently than as an intersection of the <span>$x$</span>-axis with a specific tangent line, rather we can think of it as an intersection of the <span>$x$</span>-axis with <em>a</em> line <span>$ax + by = c$</span> with <em>two</em> constraints:</p><ul><li>the point <span>$(x_n, f(x_n))$</span> is on the line</li><li>the line matches the slope of the tangent line at that point (a tangency condition)</li></ul><p>Combined, these say that the constrained line has slope <span>$f&#39;(x_n)$</span> and goes through the point <span>$(x_n, f(x_n))$</span>, so is the tangent line; Newton&#39;s method follows.</p><p>When a quadratic approximation to the graph of <span>$f(x)$</span> is chosen <em>at</em> <span>$(x_n, f(x_n))$</span> other algorithms become possible.</p><p><a href="https://doi.org/10.1016/S0377-0427(03)00420-5">Geometric constructions of iterative functions to solve nonlinear equations</a> by Amat, Busquier, and Gutiérrez has a systematic approach we follow.</p><h3 id="Euler-method"><a class="docs-heading-anchor" href="#Euler-method">Euler method</a><a id="Euler-method-1"></a><a class="docs-heading-anchor-permalink" href="#Euler-method" title="Permalink"></a></h3><p>Consider the quadratic expression <span>$y + ax^2 + bx + c = 0$</span>. Assuming the expression goes through the point <span>$(x_n, f(x_n))$</span> and the tangency conditions: <span>$y&#39;(x) = f&#39;(x)$</span> <strong>and</strong> <span>$y&#39;&#39;(x) = f&#39;&#39;(x)$</span>, we get the second-order Taylor polynomial <span>$y(x) = f(x_n) + f&#39;(x_n)(x-x_n) + f&#39;&#39;(x_n)/2 \cdot(x-x_n)^2$</span> as the solution.</p><p>Let</p><p class="math-container">\[L_f(x) = \frac{f(x)/f&#39;(x)}{f&#39;(x)/f&#39;&#39;(x)} = \frac{f(x)f&#39;&#39;(x)}{f&#39;(x)^2}.\]</p><p>Then the intersection of the curve with the <span>$x$</span>-axis can be represented as:</p><p class="math-container">\[x_{n+1} = x_n - \frac{2}{1 + \sqrt{1 - 2L_f(x_n)}} \cdot \frac{f(x_n)}{f&#39;(x_n)}.\]</p><p>This is known as Euler&#39;s method or the irrational Halley method and implemented in <code>Roots.IrrationalHalley()</code>:</p><pre><code class="language-julia hljs">L_f(x) = f(x) * f&#39;&#39;(x) / (f&#39;(x))^2

x0 = 1.4
x1 = x0 - 2 / (1 + sqrt(1 - 2L_f(x0))) * f(x0)/f&#39;(x0)
t2(x) = f(x0) + f&#39;(x0)*(x-x0) + f&#39;&#39;(x0)/2 * (x - x0)^2

a, b = 1.1, 1.5
p = plot(f, a, b; legend=false, linewidth=3)
plot!(zero)
plot!(t2; color=:red, linewidth=3)

scatter!([x0, x1], [0,0]; markercolor=:blue)
annotate!([(x0,0,&quot;x0&quot;, :bottom), (x1, 0, &quot;x1&quot;, :bottom)])
scatter!([x0], [f(x0)]; markercolor=:blue)

scatter!([α],[0]; markercolor=:blue)
annotate!([(α, 0, &quot;α&quot;, :top)])

p</code></pre><img src="40c5e8cc.svg" alt="Example block output"/><h3 id="Halley&#39;s-method"><a class="docs-heading-anchor" href="#Halley&#39;s-method">Halley&#39;s method</a><a id="Halley&#39;s-method-1"></a><a class="docs-heading-anchor-permalink" href="#Halley&#39;s-method" title="Permalink"></a></h3><p>The general form of a quadratic equation <span>$Ax^2 + By^2 + Cxy + Dx + Ey + F = 0$</span> is specialized above by setting <span>$B=C=0$</span> and <span>$E=1$</span> and then imposing the point <span>$(x_n, f(x_n))$</span> as a solution along with tangency conditions. The famous Halley&#39;s method can be seen as the specialization to a hyperbola: <span>$axy + y + bx + c = 0$</span>. This yields the curve</p><p class="math-container">\[y  - (f(x_n)  + f&#39;(x_n)(x-x_n) + \frac{f&#39;&#39;(x_n)}{2f&#39;(x_n)}(x-x_n) \cdot (y-f(x_n))) = 0\]</p><p>and the iterative algorithm:</p><p class="math-container">\[x_{n+1} = x_n - \frac{2}{2 - L_f(x_n)} \cdot \frac{f(x_n)}{f&#39;(x_n)}.\]</p><p>We can visualize, as follows, using a contour plot to represent the hyperbola.</p><pre><code class="language-julia hljs">x1 = x0 - 2 / (2 - L_f(x0)) * f(x0)/f&#39;(x0)
F(x,y) = y - f(x0) - f&#39;(x0)*(x-x0) - f&#39;&#39;(x0)/(2f&#39;(x0)) * (x-x0) * (y-f(x0))

a, b = 1.1, 1.5
p = plot(f, a, b; legend=false, linewidth=3)
plot!(zero)
xs, ys = range(a, b, length=50), range(f(a), f(b), length=50);
zs = [F(x,y) for y ∈ ys, x ∈ xs];
contour!(xs, ys, zs; levels = [0], color=:red, linewidth=3)

scatter!([x0, x1], [0,0]; markercolor=:blue)
annotate!([(x0,0,&quot;x0&quot;, :bottom), (x1, 0, &quot;x1&quot;, :bottom)])

scatter!([x0], [f(x0)]; markercolor=:blue)

scatter!([α],[0]; markercolor=:blue)
annotate!([(α, 0, &quot;α&quot;, :top)])

p</code></pre><img src="1b2e0e58.svg" alt="Example block output"/><p><code>Roots.Halley()</code> provides an implementation.</p><h3 id="Chebyshev&#39;s-method"><a class="docs-heading-anchor" href="#Chebyshev&#39;s-method">Chebyshev&#39;s method</a><a id="Chebyshev&#39;s-method-1"></a><a class="docs-heading-anchor-permalink" href="#Chebyshev&#39;s-method" title="Permalink"></a></h3><p>Chebyshev&#39;s method uses an inverse quadratic fit, specialized with <span>$ay^2 + y + bx + c = 0$</span>, to compute the next iterate. It can be expressed via:</p><p class="math-container">\[\frac{-f&#39;&#39;(x_n)}{2f&#39;(x_n)^2}(y - f(x_n))^2 + y - f(x_n) - f&#39;(x_n)(x-x_n) = 0\]</p><p>and the algorithm becomes:</p><p class="math-container">\[x_{n+1} = x_n - (1 + \frac{1}{2} L_f(x_n)) \frac{f(x_n)}{f&#39;(x_n)}.\]</p><p>This is visualized in a similar manner as the last example:</p><pre><code class="language-julia hljs">x1 = x0 - (1 + 1/2 * L_f(x0)) * f(x0) / f&#39;(x0)
F(x, y) = -f&#39;&#39;(x0)/(2f&#39;(x0)^2) * (y-f(x0))^2 + y - f(x0)  - f&#39;(x0) * (x- x0)

a, b = 1.1, 1.5
p = plot(f, a, b; legend=false, linewidth=3)
plot!(zero)
xs, ys = range(a, b, length=50), range(f(a), f(b), length=50);
zs = [F(x,y) for y ∈ ys, x ∈ xs];
contour!(xs, ys, zs; levels = [0], color=:red, linewidth=3)

scatter!([x0, x1], [0,0]; markermarkercolor=:blue)
annotate!([(x0,0,&quot;x0&quot;, :bottom), (x1, 0, &quot;x1&quot;, :bottom)])

scatter!([x0], [f(x0)]; markermarkercolor=:blue)

scatter!([α],[0]; markermarkercolor=:blue)
annotate!([(α, 0, &quot;α&quot;, :top)])

p</code></pre><img src="798da204.svg" alt="Example block output"/><p><code>Roots.InverseQuadratic()</code> provides an implementation; <code>Roots.ChebyshevLike()</code> an accelerated version.</p><p>Amat, Busquier, and Gutierrez also consider the hyperbola</p><p class="math-container">\[ay^2 + bxy + y + cx + d = 0\]</p><p>As there are <span>$4$</span> unknowns and only <span>$3$</span> constraints, the solution will depend on a parameter, they call <span>$b_n,$</span> yielding:</p><p class="math-container">\[\begin{align*}
c_n &amp;= -f&#39;(x_n)\\
a_n &amp;= -\frac{f&#39;&#39;(x_n)}{2f&#39;(x_n)^2} - \frac{b_n}{f&#39;(x_n)}\\
0 &amp;= x - x_n  + (y-f(x_n)) \frac{1 + a_n \cdot (y - f(x_n))}{b_n\cdot (y-f(x_n)) + c_n}
\end{align*}\]</p><p>This gives the algorithm:</p><p class="math-container">\[x_{n+1} = x_n - \left(1 + \frac{1}{2}\frac{L_f(x_n)}{1 + b_n \cdot (f(x_n)/f&#39;(x_n))}\right) \cdot \frac{f(x_n)}{f&#39;(x_n)}.\]</p><p>Newton&#39;s method is recovered by letting <span>$b_n \rightarrow 0$</span>, Chebyshev&#39;s method is when <span>$b_n=0$</span>, Halley&#39;s method is when <span>$b_n = -f&#39;&#39;(x_n)/(2f&#39;(x_n))$</span>.</p><p>The super-Halley method is when <span>$b_n = -f&#39;&#39;(x_n)/f&#39;(x_n)$</span>. We can visualize this:</p><pre><code class="language-julia hljs">cn = -f&#39;(x0)
bn = -f&#39;&#39;(x0)/f&#39;(x0)
an = -f&#39;&#39;(x0)/(2f&#39;(x0)^2) - bn/f&#39;(x0)
x1 = x0 - (1 + 1/2 * (L_f(x0) / (1 + bn * f(x0) / f&#39;(x0)))) * f(x0)/f&#39;(x0)
F(x, y) = x - x0 + (y-f(x0)) * (1 + an * (y - f(x0))) / (bn * (y - f(x0)) + cn)


a, b= 1.1, 1.5
p = plot(f, a, b; legend=false, linewidth=3)
plot!(zero)
xs, ys = range(a, b, length=50), range(f(a), f(b), length=50);
zs = [F(x,y) for y ∈ ys, x ∈ xs];
contour!(xs, ys, zs; levels = [0], color=:red, linewidth=3)

scatter!([x0, x1], [0,0]; markercolor=:blue)
annotate!([(x0,0,&quot;x0&quot;, :bottom), (x1, 0, &quot;x1&quot;, :bottom)])
scatter!([x0], [f(x0)]; markercolor=:blue)

scatter!([α],[0]; markercolor=:blue)
annotate!([(α, 0, &quot;α&quot;, :top)])

p</code></pre><img src="9823be64.svg" alt="Example block output"/><p><code>Roots.SuperHalley()</code> provides an implementation.</p><p>The author&#39;s discuss using different osculating curves, such as a cubic equation. As they mention, all their methods take the form (using <a href="https://en.wikipedia.org/wiki/Big_O_notation">big O notation</a>):</p><p class="math-container">\[x_{n+1} = x_n - (1 + \frac{1}{2}L_f(x_n) + \mathcal{O}(L_f(x_n)^2)) \cdot \frac{f(x_n)}{f&#39;(x_n)}.\]</p><p>The algorithms implemented in the <code>Roots.LithBoonkkampIJzerman{S,D}()</code> methods use a differential equations approach to evaluate the inverse of <span>$f(x)$</span> at <span>$0$</span>. The methods all find inverse polynomial approximations to <span>$f^{-1}$</span>. The methods for <span>$D=0$</span>, are, as seen, for <span>$S=2$</span> an inverse secant line, for <span>$S=3$</span> an inverse quadratic approximation, for <span>$S=4$</span> an inverse cubic; when <span>$S=1$</span> and <span>$D=1$</span> Euler&#39;s method turns into Newton&#39;s method, for <span>$S=1$</span> and <span>$D=2$</span> the inverse quadratic or Chebyshev method is used.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Roots.jl</a><a class="docs-footer-nextpage" href="../reference/">Reference/API »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="auto">Automatic (OS)</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.1.0 on <span class="colophon-date" title="Friday 29 September 2023 13:32">Friday 29 September 2023</span>. Using Julia version 1.9.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
