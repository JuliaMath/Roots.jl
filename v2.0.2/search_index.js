var documenterSearchIndex = {"docs":
[{"location":"roots/#An-overview-of-Roots","page":"An overview of Roots","title":"An overview of Roots","text":"","category":"section"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"The Roots package contains simple routines for finding zeros of continuous scalar functions of a single real variable.  A zero of f is a value c where f(c) = 0.  The basic interface is through the function find_zero, which through multiple dispatch can handle many different cases.","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"The NonlinearSolve package provides an alternative.","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"In the following, we will use  ForwardDiff to take derivatives.","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"julia> using Roots, ForwardDiff\n","category":"page"},{"location":"roots/#Bracketing","page":"An overview of Roots","title":"Bracketing","text":"","category":"section"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"For a function f (univariate, real-valued) a bracket is a pair $ a < b $ for which f(a) cdot f(b)  0. That is the function values have different signs at a and b. If f is a continuous function this ensures (Bolzano) there will be a zero in the interval ab.  If f is not continuous, then there must be a point c in ab where the function \"jumps\" over 0.","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"Such values can be found, up to floating point round off. That is, given f(a) * f(b) < 0, a value c with a < c < b can be found where either f(c) == 0.0 or  f(prevfloat(c)) * f(c) < 0 or f(c) * f(nextfloat(c)) < 0.","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"To illustrate, consider the function f(x) = cos(x) - x. From trigonometry  we can see readily infer that 0pi2 is a bracket.","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"The Roots package includes the bisection algorithm through find_zero. We use a structure for which extrema returns (a,b) with a < b, such as a vector or tuple, to specify the initial condition and Bisection() to specify the algorithm:","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"julia> f(x) = cos(x) - x;\n\njulia> x = find_zero(f, (0, pi/2), Bisection())\n0.7390851332151607\n\njulia> x, f(x)\n(0.7390851332151607, 0.0)","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"For this function we see that f(x) is 0.0.","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"Functions may be parameterized. The following is a similar function as above, still having (0 pi2) as  bracket for p0. By passing in values of p to find_zero, different, related problems may be solved.","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"julia> g(x, p=1) = cos(x) - x/p;\n\njulia> x0, M = (0, pi/2), Bisection()\n((0, 1.5707963267948966), Bisection())\n\njulia> find_zero(g, x0, M) # as before, solve cos(x) - x = 0 using default p=1\n0.7390851332151607\n\njulia> find_zero(g, x0, M; p=2) # solves cos(x) - x/2 = 0\n1.0298665293222589","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"Next consider f(x) = sin(x). A known zero is pi. Trigonometry tells us that pi2 3pi2 will be a bracket.  The calling pattern for find_zero is find_zero(f, x0, M; kwargs...), where kwargs can specify details about parameters for the problem or tolerances for the solver.  In this call Bisection() is not specified, as it will be the default when the initial value is specified as a pair of numbers:","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"julia> f(x) = sin(x);\n\njulia> x = find_zero(f, (pi/2, 3pi/2))\n3.141592653589793\n\njulia> x, f(x)\n(3.141592653589793, 1.2246467991473532e-16)\n","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"This value of x does not exactly produce a zero, however, it is as close as can be:","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"julia> f(prevfloat(x)) * f(x) < 0.0 || f(x) * f(nextfloat(x)) < 0.0\ntrue\n","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"That is, at x the function is changing sign.","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"From a mathematical perspective, a zero is guaranteed for a continuous function. However, the computer algorithm doesn't assume continuity, it just looks for changes of sign. As such, the algorithm will  identify discontinuities, not just zeros. For example:","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"julia> find_zero(x -> 1/x, (-1, 1))\n0.0\n","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"The endpoints and function values can even be infinite for the default Bisection algorithm over the standard floating point types:","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"julia> find_zero(x -> Inf*sign(x), (-Inf, Inf))  # Float64 only\n0.0\n","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"The basic algorithm used for bracketing when the values are simple floating point values is a modification of the bisection method, where the midpoint is taken over the bit representation of a and b.","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"For big float values, bisection is the default (with non-zero tolerances), but its use is definitely not suggested. Simple bisection over BigFloat values can take many more iterations. For the problem of finding a zero of sin in the interval (big(3), big(4)), the default bisection takes 252 iterations, whereas the A42 method takes 4.","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"The algorithms of Alefeld, Potra, and Shi and the well known algorithm of Brent, also start with a bracketing algorithm. For many problems these will take far fewer steps than the bisection algorithm to reach convergence. These may be called directly. For example,","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"julia> find_zero(sin, (3,4), A42())\n3.141592653589793","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"By default, bisection will converge to machine tolerance. This may provide more accuracy than desired. A tolerance may be specified to terminate early, thereby utilizing fewer resources. For example, this uses 4 steps to reach accuracy to 116 (without specifying xatol it uses 53 steps):","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"julia> rt = find_zero(sin, (3.0, 4.0), xatol=1/16)\n3.125\n\njulia> rt - pi\n-0.016592653589793116\n","category":"page"},{"location":"roots/#Non-bracketing-problems","page":"An overview of Roots","title":"Non-bracketing problems","text":"","category":"section"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"Bracketing methods have guaranteed convergence, but in general may require many more function calls than are otherwise needed to produce an answer.  If a good initial guess is known, then the find_zero function provides an interface to some different iterative algorithms that are more efficient. Unlike bracketing methods, these algorithms may not converge to the desired root if the initial guess is not well chosen.","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"The default algorithm is modeled after an algorithm used for HP-34 calculators. This algorithm is designed to be more forgiving of the quality of the initial guess at the cost of possibly performing  more steps than other algorithms, as if the algorithm encounters a bracket, a bracketing method will be used (an efficient one, though).","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"For example, the answer to our initial problem is visibly seen from a graph to be near 1. Given this, the zero is found through:","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"julia> f(x) = cos(x) - x;\n\njulia> x = find_zero(f , 1)\n0.7390851332151607\n\njulia> x, f(x)\n(0.7390851332151607, 0.0)\n","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"For the polynomial f(x) = x^3 - 2x - 5, an initial guess of 2 seems reasonable:","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"julia> f(x) = x^3 - 2x - 5;\n\njulia> x = find_zero(f, 2)\n2.0945514815423265\n\njulia> x, f(x), sign(f(prevfloat(x)) * f(nextfloat(x)))\n(2.0945514815423265, -8.881784197001252e-16, -1.0)\n","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"For even more precision, BigFloat numbers can be used","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"julia> x = find_zero(sin, big(3))\n3.141592653589793238462643383279502884197169399375105820974944592307816406286198\n\njulia> x, sin(x), x - pi\n(3.141592653589793238462643383279502884197169399375105820974944592307816406286198, 1.096917440979352076742130626395698021050758236508687951179005716992142688513354e-77, 0.0)\n","category":"page"},{"location":"roots/#Higher-order-methods","page":"An overview of Roots","title":"Higher order methods","text":"","category":"section"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"The default call to find_zero uses a first order method and then possibly bracketing, which potentially involves more function calls than necessary. There may be times where a more efficient algorithm is sought. For such, a higher-order method might be better suited. There are algorithms Order1 (secant method), Order2 (Steffensen), Order5, Order8, and Order16. The order 1 or 2 methods are generally quite efficient in terms of steps needed over floating point values. The even higher order ones are potentially useful when more precision is used. These algorithms are accessed by specifying the method after the initial starting point:","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"julia> f(x) = 2x - exp(-x);\n\njulia> x = find_zero(f, 1, Order1())\n0.3517337112491958\n\njulia> x, f(x)\n(0.3517337112491958, -1.1102230246251565e-16)\n","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"Similarly,","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"julia> f(x) = (x + 3) * (x - 1)^2;\n\njulia> x = find_zero(f, -2, Order2())\n-3.0\n\njulia> x, f(x)\n(-3.0, 0.0)\n","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"julia> x = find_zero(f, 2, Order8())\n1.0000000131073141\n\njulia> x, f(x)\n(1.0000000131073141, 6.87206736323862e-16)\n","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"Starting at 2 converges to 1, showing that zeros need not be simple zeros to be found. A simple zero, c, has f(x) = (x-c) cdot g(x) where g(c) neq 0. Generally speaking, non-simple zeros are expected to take many more function calls, as the methods are no longer super-linear. This is the case here, where Order2 uses 51 function calls, Order8 uses 42, and Order0 takes  80. The Roots.Order2B method is useful when a multiplicity is expected; on this problem it takes 17 function calls.","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"To investigate an algorithm and its convergence, the argument verbose=true may be specified. A Roots.Tracks object can be used to store the intermediate values.","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"For some functions, adjusting the default tolerances may be necessary to achieve convergence. The tolerances include atol and rtol, which are used to check if f(x_n) approx 0; xatol and xrtol, to check if x_n approx x_n-1; and maxiters  to limit the number of iterations in the algorithm.","category":"page"},{"location":"roots/#Classical-methods","page":"An overview of Roots","title":"Classical methods","text":"","category":"section"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"The package provides some classical methods for root finding, such as Roots.Newton, Roots.Halley, and Roots.Schroder. (Currently these are not exported, so must be prefixed with the package name to be used.) We can see how each works on a problem studied by Newton. Newton's method uses the function and its derivative:","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"julia> f(x) = x^3 - 2x - 5;\n\njulia> fp(x) = 3x^2 - 2;\n\njulia> x = Roots.find_zero((f, fp), 2, Roots.Newton())\n2.0945514815423265\n\njulia> x, f(x)\n(2.0945514815423265, -8.881784197001252e-16)\n","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"The functions are specified using a tuple, or through a function returning (f(x), f(x)/f'(x)). The latter is convenient when f' is easily computed when f is, but otherwise may be expensive to compute.","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"Halley's method has cubic convergence, as compared to Newton's quadratic convergence. It uses the second derivative as well:","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"julia> fpp(x) = 6x;\n\njulia> x = Roots.find_zero((f, fp, fpp), 2, Roots.Halley())\n2.0945514815423265\n\njulia> x, f(x), sign(f(prevfloat(x)) * f(nextfloat(x)))\n(2.0945514815423265, -8.881784197001252e-16, -1.0)","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"(Halley's method takes 3 steps, Newton's 4, but Newton's uses 5 function calls to Halley's 10.)","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"For many functions, their derivatives can be computed automatically. The ForwardDiff package provides a means. Here we define an operator D to compute a derivative:","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"julia> function D(f, n::Int=1)\n           n <= 0 && return f\n           n == 1 && return x -> ForwardDiff.derivative(f,float(x))\n           D(D(f,1),n-1)\n       end\nD (generic function with 2 methods)\n\njulia> dfᵏs(f,k) = ntuple(i->D(f,i-1), Val(k+1)) # (f, f′, f′′, …)\ndfᵏs (generic function with 1 method)","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"julia> find_zero((f,D(f)), 2, Roots.Newton())\n2.0945514815423265\n","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"Or, for Halley's method:","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"julia> find_zero((f, D(f), D(f,2)), 2, Roots.Halley())\n2.0945514815423265\n","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"The family of solvers implemented in Roots.LithBoonkkampIJzerman(S,D) where S is the number of prior points used to generate the next, and D is the number of derivatives used, has both the secant method (S=2, D=0) and Newton's method (S=1, D=1) as members, but also provides others. By adding more memory or adding more derivatives the convergence rate increases, at the expense of more complicated expressions or more function calls per step.","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"julia> find_zero(dfᵏs(f, 0), 2, Roots.LithBoonkkampIJzerman(3,0)) # like secant\n2.0945514815423265\n\njulia> find_zero(dfᵏs(f, 1), 2, Roots.LithBoonkkampIJzerman(2,1)) # like Newton\n2.0945514815423265\n\njulia> find_zero(dfᵏs(f, 2), 2, Roots.LithBoonkkampIJzerman(2,2)) # like Halley\n2.0945514815423265","category":"page"},{"location":"roots/#The-problem-algorithm-solve-interface","page":"An overview of Roots","title":"The problem-algorithm-solve interface","text":"","category":"section"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"The problem-algorithm-solve interface is a pattern popularized in Julia by the DifferentialEquations.jl suite of packages. The pattern consists of setting up a problem then solving the problem by specifying an algorithm. This is very similar to what is specified in the find_zero(f, x0, M) interface where f and x0 specify the problem, M the algorithm, and find_zero calls the solver.","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"To break this up into steps, Roots has methods ZeroProblem and init, solve, and solve! from the CommonSolve.jl package.","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"Consider solving sin(x) = 0 using the Secant method starting with the interval 34.","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"julia> f(x) = sin(x);\n\njulia> x0 = (3, 4)\n(3, 4)\n\njulia> M = Secant()\nSecant()\n\njulia> Z = ZeroProblem(f, x0)\nZeroProblem{typeof(f), Tuple{Int64, Int64}}(f, (3, 4))\n\njulia> solve(Z, M)\n3.141592653589793","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"Changing the method is easy:","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"julia> solve(Z, Order2())\n3.1415926535897944","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"The solve interface works with parameterized functions, as well:","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"julia> g(x, p=1) = cos(x) - x/p;\n\njulia> Z = ZeroProblem(g, (0.0, pi/2))\nZeroProblem{typeof(g), Tuple{Float64, Float64}}(g, (0.0, 1.5707963267948966))\n\njulia> solve(Z, Secant(); p=2) # uses p=2\n1.0298665293222589\n\njulia> solve(Z, Bisection(); p=3, xatol=1/16) # p=3; uses keywords for tolerances\n1.1959535058744393","category":"page"},{"location":"roots/#Finding-critical-points","page":"An overview of Roots","title":"Finding critical points","text":"","category":"section"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"The D function, defined above, makes it straightforward to find critical points (typically where the derivative is 0 but also where it is undefined). For example, the critical point of the function f(x) = 1x^2 + x^3 x  0 near 10 is where the derivative is 0 and can be found through:","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"julia> f(x) = 1/x^2 + x^3;\n\njulia> find_zero(D(f), 1)\n0.9221079114817278","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"For more complicated expressions, D may need some technical adjustments to be employed. In this example, we have a function that models the flight of an arrow on a windy day:","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"julia> function flight(x, theta)\n         k = 1/2\n         a = 200*cosd(theta)\n         b = 32/k\n         tand(theta)*x + (b/a)*x - b*log(a/(a-x))\n       end\nflight (generic function with 1 method)","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"The total distance flown is when flight(x) == 0.0 for some x > 0: This can be solved for different theta with find_zero. In the following, we note that log(a/(a-x)) will have an asymptote at a, so we start our search at a-5:","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"julia> function howfar(theta)\n         a = 200*cosd(theta)\n         find_zero(x -> flight(x, theta), a-5)  # starting point has type determined by `theta`.\n        end\nhowfar (generic function with 1 method)","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"To visualize the trajectory if shot at 45 degrees, we would have:","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"using Roots, ForwardDiff  # hide\nusing Plots; unicodeplots()  # hide\n\nflight(x, theta) = (k = 1/2; a = 200*cosd(theta); b = 32/k; tand(theta)*x + (b/a)*x - b*log(a/(a-x)))\nhowfar(theta) = (a = 200*cosd(theta); find_zero(x -> flight(x, theta), a-5))\nhowfarp(t) = ForwardDiff.derivative(howfar,t)\n\ntheta = 45\ntstar = find_zero(howfarp, 45)\n\nplot(x -> flight(x,  theta), 0, howfar(theta))\nshow(current())  # hide","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"To maximize the range we solve for the lone critical point of howfar within reasonable starting points.","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"The automatic differentiation provided by ForwardDiff will work through a call to find_zero if the initial point has the proper type (depending on an expression of theta in this case). As we use 200*cosd(theta)-5 for a starting point, this is satisfied.","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"julia> (tstar = find_zero(D(howfar), 45)) ≈ 26.2623089\ntrue","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"This graph would show the differences in the trajectories:","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"plot(x -> flight(x, 45), 0, howfar(45))\nplot!(x -> flight(x, tstar), 0, howfar(tstar))\nshow(current())  # hide","category":"page"},{"location":"roots/#Potential-issues","page":"An overview of Roots","title":"Potential issues","text":"","category":"section"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"The higher-order methods are basically various derivative-free versions of Newton's method (which has update step x - f(x)f(x)). For example, Steffensen's method (Order2()) essentially replaces f(x) with (f(x + f(x)) - f(x))f(x). This is a forward-difference approximation to the derivative with \"h\" being f(x), which presumably is close to 0 already. The methods with higher order combine this with different secant line approaches that minimize the number of function calls. These higher-order methods can be susceptible to some of the usual issues found with Newton's method: poor initial guess, small first derivative, or large second derivative near the zero.","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"When the first derivative is near 0, the value of the next step can be quite different, as the next step generally tracks the intersection point of the tangent line. We see that starting at a pi2 causes this search to be problematic:","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"julia> try  find_zero(sin, pi/2, Order1()) catch err  \"Convergence failed\" end\n\"Convergence failed\"","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"(Whereas, starting at pi/2 + 0.3–where the slope of the tangent is sufficiently close to point towards pi–will find convergence at pi.)","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"For a classic example where a large second derivative is the issue, we have f(x) = x^13:","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"julia> f(x) = cbrt(x);\n\njulia> x = try  find_zero(f, 1, Order2())  catch err  \"Convergence failed\" end\t# all of 2, 5, 8, and 16 fail or diverge towards infinity\n\"Convergence failed\"\n","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"However, the default finds the root here, as a bracket is identified:","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"julia> x = find_zero(f, 1)\n0.0\n\njulia> x,  f(x)\n(0.0, 0.0)\n","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"Finally, for many functions, all of these methods need a good initial guess. For example, the polynomial function f(x) = x^5 - x - 1 has its one zero near 116. If we start far from it, convergence may happen, but it isn't guaranteed:","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"julia> f(x) = x^5 - x - 1;\n\njulia> x0 = 0.1\n0.1\n\njulia> try find_zero(f, x0)  catch err  \"Convergence failed\" end\n\"Convergence failed\"\n","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"A graph shows the issue. Running the following shows 15 steps of Newton's method, the other algorithms being somewhat similar:","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"f(x) = x^5 - x - 1; # hide\nD(f) = x -> ForwardDiff.derivative(f,float(x)) # hide\nxs = [0.1] # x0\nn = 15\nfor i in 1:(n-1) push!(xs, xs[end] - f(xs[end])/D(f)(xs[end])) end\nys = [zeros(Float64,n)';map(f, xs)'][1:2n]\nxs = xs[repeat(collect(1:n), inner=[2], outer=[1])]\nplot(f, -1.25, 1.5, linewidth=3, legend=false)\nplot!(zero, -1.25, 1.5, linewidth=3)\nplot!(xs, ys)\nshow(current())  # hide","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"Only a few of the steps are discernible, as the function's relative maximum causes a trap for this algorithm. Starting to the right of the relative minimum–nearer the zero–would avoid this trap. The default method employs a trick to bounce out of such traps, though it doesn't always work.","category":"page"},{"location":"roots/#Tolerances","page":"An overview of Roots","title":"Tolerances","text":"","category":"section"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"Mathematically solving for a zero of a nonlinear function may be impossible, so numeric methods are utilized. However, using floating point numbers to approximate the real numbers leads to some nuances.","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"For example, consider the polynomial f(x) = (3x-1)^5 with one zero at 13 and its equivalent expression f1(x) = -1 + 15cdot x - 90cdot x^2 + 270cdot x^3 - 405cdot x^4 + 243cdot x^5. Mathematically these are the same, however not so when evaluated in floating point. Here we look at the 21 floating point numbers near 13:","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"julia> f(x) = (3x-1)^5;\n\njulia> f1(x) =  -1 + 15*x - 90*x^2 + 270*x^3 - 405*x^4 + 243*x^5;\n\njulia> above = accumulate((x,y) -> nextfloat(x), 1:10, init=1/3);\n\njulia> below = accumulate((x,y) -> prevfloat(x), 1:10, init=1/3);\n\njulia> ns = sort([below...,1/3, above...]);\n\njulia> maximum(abs.(f.(ns) - f1.(ns))) < 1e-14\ntrue","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"We see the function values are close for each point, as the maximum difference is like 10^-15. This is roughly as expected, where even one addition may introduce a relative error as big as 2cdot 10^-16 and here there are several such.","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"note: Note\n(These values are subject to the vagaries of floating point evaluation, so may differ depending on the underlying computer architecture.)","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"Generally this variation is not even a thought, as the differences are generally negligible, but when we want to identify if a value is zero, these small differences might matter. Here we look at the signs of the function values for a run of the above:","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"julia> fs = sign.(f.(ns));\n\njulia> f1s = sign.(f1.(ns));\n\njulia> [ns.-1/3 fs f1s]\n21×3 Matrix{Float64}:\n -5.55112e-16  -1.0  -1.0\n -4.996e-16    -1.0  -1.0\n -4.44089e-16  -1.0   1.0\n -3.88578e-16  -1.0   1.0\n -3.33067e-16  -1.0   1.0\n -2.77556e-16  -1.0   1.0\n -2.22045e-16  -1.0  -1.0\n -1.66533e-16  -1.0  -1.0\n -1.11022e-16  -1.0   1.0\n -5.55112e-17  -1.0   1.0\n  0.0           0.0  -1.0\n  5.55112e-17   0.0   1.0\n  1.11022e-16   1.0   1.0\n  1.66533e-16   1.0  -1.0\n  2.22045e-16   1.0  -1.0\n  2.77556e-16   1.0  -1.0\n  3.33067e-16   1.0   1.0\n  3.88578e-16   1.0   1.0\n  4.44089e-16   1.0   1.0\n  4.996e-16     1.0  -1.0\n  5.55112e-16   1.0   0.0","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"Parsing this shows a few surprises. First, there are two zeros of f(x) identified–not just one as expected mathematically–the floating point value of 1/3 and the next largest floating point number.","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"julia> findall(iszero, fs)\n2-element Vector{Int64}:\n 11\n 12","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"For f1(x) there is only one zero, but it isn't the floating point value for 1/3 but rather 10 floating point numbers away.","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"julia> findall(iszero, f1s)\n1-element Vector{Int64}:\n 21","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"Further, there are several sign changes of the function values for f1s:","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"julia> findall(!iszero,diff(sign.(f1s)))\n9-element Vector{Int64}:\n  2\n  6\n  8\n 10\n 11\n 13\n 16\n 19\n 20\n","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"There is no guarantee that a zero will be present, but for a mathematical function that changes sign, there will be at least one sign change.","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"With this in mind, an exact zero of f would be either where iszero(f(x)) is true or where the function has a sign change (either f(x)*f(prevfloat(x))<0 or f(x)*f(nextfloat(x)) < 0).","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"As mentioned, the default Bisection() method of find_zero identifies such zeros for f provided an initial bracketing interval is specified when Float64 numbers are used.  However, if a mathematical function does not cross the x axis at a zero, then there is no guarantee the floating point values will satisfy either of these conditions.","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"Now consider the function f(x) = exp(x)-x^4. The valuex=8.613169456441398 is a zero in this sense, as there is a change of sign:","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"julia> f(x) = exp(x) - x^4;\n\njulia> F(x) = sign(f(x));\n\njulia> x=8.613169456441398\n8.613169456441398\n\njulia> F(prevfloat(x)), F(x), F(nextfloat(x))\n(-1.0, -1.0, 1.0)\n","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"However, the value of f(x) is not as small as one might initially expect for a zero:","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"julia> f(x), abs(f(x)/eps(x))\n(-2.7284841053187847e-12, 1536.0)\n","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"The value x is an approximation to the actual mathematical zero, call it x. There is a difference between f(x) (the mathematical answer) and f(x) (the floating point answer). Roughly speaking we expect f(x) to be about f(x) + f(x)cdot delta, where delta is the difference between x and x. This will be on the scale of abs(x) * eps(), so all told we expect an answer to be in the range of 0 plus or minus this value:","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"julia> fp(x) = exp(x) - 4x^3; # the derivative\n\njulia> fp(x) * abs(x) * eps()\n5.637565490466956e-12\n","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"which is about what we see.","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"Bisection can be a slower method than others. For Float64 values, Bisection() takes no more than 64 steps, but other methods may be able to converge to a zero in 4-5 steps (assuming good starting values are specified).","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"When fewer function calls are desirable, then checking for an approximate zero may be preferred over assessing if a sign change occurs, as generally that will take two additional function calls per step. Besides, a sign change isn't guaranteed for all zeros. An approximate zero would be one where f(x) approx 0.","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"By the above, we see that we must consider an appropriate tolerance. The first example shows differences in floating point evaluations from the mathematical ones might introduce errors on the scale of eps regardless of the size of x. As seen in the second example, the difference between the floating point approximation to the zero and the zero introduces a potential error proportional to the size of x. So a tolerance might consider both types of errors. An absolute tolerance is used as well as a relative tolerance, so a check might look like:","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"abs(f(x)) < max(atol, abs(x) * rtol)","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"This is different from Julia's isapprox(f(x), 0.0), as that would use abs(f(x)) as the multiplier, which renders a relative tolerance useless for this question.","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"One issue with relative tolerances is that for functions with sublinear growth, extremely large values will be considered zeros. Returning to an earlier example, with Thukral8 we have a misidentified zero:","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"julia> find_zero(cbrt, 1, Roots.Thukral8())\n1.725042287244107e23\n","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"The algorithm rapidly marches off towards infinity so the relative tolerance approx x cdot epsilon used to check if f(x) approx 0 is large compared to the far-from zero f(x).","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"Either the users must be educated about this possibility, or the relative tolerance should be set to 0. In that case, the absolute tolerance must be relatively generous.  A conservative choice of absolute tolerance might be sqrt(eps()), or about 1e-8, essentially the one made in SciPy.","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"Though this tolerance won't be able to work really large values:","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"julia> find_zero(x -> sqrt(eps()) - eps(x), (0,Inf))\n9.981132799999999e7","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"This is not the choice made in Roots. The fact that bisection can produce zeros as exact as possible, and the fact that the error in function evaluation, f(x)xepsilon, is not typically on the scale of 1e-8, leads to a desire for more precision, if available.","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"In Roots, the faster algorithms use a check on both the size of f(xn) and the size of the difference between the last two xn values. The check on f(xn) is done with a tight tolerance, as is the check on x_n approx x_n-1. If the function values get close to zero, an approximate zero is declared. Further, if the x values get close to each other and the function value is close to zero with a relaxed tolerance, then an approximate zero is declared. In practice this seems to work reasonably well. The relaxed tolerance uses the cube root of the absolute and relative tolerances.","category":"page"},{"location":"roots/#Searching-for-all-zeros-in-an-interval","page":"An overview of Roots","title":"Searching for all zeros in an interval","text":"","category":"section"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"The methods described above are used to identify one of possibly several zeros.  The find_zeros function searches the interval (ab) for all zeros of a function f. It is straightforward to use:","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"julia> f(x) = exp(x) - x^4;\n\njulia> a, b = -10, 10\n(-10, 10)\n\njulia> zs = find_zeros(f, a, b)\n3-element Vector{Float64}:\n -0.8155534188089606\n  1.4296118247255556\n  8.613169456441398\n","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"The search interval, (ab), is specified either through two arguments or through a single argument using a structure, such as a tuple or vector, where extrema returns two distinct values in increasing order.  It is assumed that neither endpoint is a zero.","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"The algorithm used to search for all zeros in an interval is confounded by a few things:","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"too many zeros in the interval (ab)\nnearby zeros (\"nearby\" depends on the size of (ab), should this be very wide)","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"The algorithm is adaptive, so that it can succeed when there are many zeros, but it may be necessary to increase no_pts from the default of 12, at the cost of possibly taking longer for the search.","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"Here the algorithm identifies all the zeros, despite there being several:","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"julia> f(x) = cos(x)^2 + cos(x^2); a,b = 0, 10;\n\njulia> rts = find_zeros(f, a, b);\n\njulia> length(rts)\n32","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"For nearby zeros, the algorithm does pretty well, though it isn't perfect.","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"Here we see for f(x) = sin(1x)–with infinitely many zeros around 0–it finds many:","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"julia> f(x) = iszero(x) ? NaN : sin(1/x);  # avoid sin(Inf) error\n\njulia> rts = find_zeros(f, -1, 1);\n\njulia> length(rts) # 88 zeros identified\n88","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"The function, f(x) = (x-05)^3 cdot (x-0499)^3, looks too much like g(x) = x^6 to find_zeros for success, as the two zeros are very nearby:","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"julia> f(x) =  (x-0.5)^3 * (x-0.499)^3;\n\njulia> find_zeros(f, 0, 1)\n1-element Vector{Float64}:\n 0.5","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"The issue here isn't just that the algorithm can't identify zeros within 0001 of each other, but that the high power makes many nearby values approximately zero.","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"The algorithm will have success when the powers are smaller","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"julia> f(x) =  (x-0.5)^2 * (x-0.499)^2;\n\njulia> find_zeros(f, 0, 1)\n2-element Vector{Float64}:\n 0.49899999999999994\n 0.5\n","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"It can have success for closer pairs of zeros:","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"julia> f(x) = (x-0.5) * (x - 0.49999);\n\njulia> find_zeros(f, 0, 1)\n2-element Vector{Float64}:\n 0.49999\n 0.5\n","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"Combinations of large (even) multiplicity zeros or very nearby zeros, can lead to misidentification.","category":"page"},{"location":"roots/#IntervalRootFinding","page":"An overview of Roots","title":"IntervalRootFinding","text":"","category":"section"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"The IntervalRootFinding package rigorously identifies isolating intervals for the zeros of a function. This example, from that package's README, is used to illustrate the differences:","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"julia> using IntervalArithmetic, IntervalRootFinding, Roots\n\njulia> f(x) = sin(x) - 0.1*x^2 + 1\nf (generic function with 1 method)\n\njulia> rts = roots(f, -10..10)\n4-element Vector{Root{Interval{Float64}}}:\n Root([3.14959, 3.1496], :unique)\n Root([-4.42654, -4.42653], :unique)\n Root([-3.10682, -3.10681], :unique)\n Root([-1.08205, -1.08204], :unique)\n\njulia> find_zeros(f, -10, 10)\n4-element Vector{Float64}:\n -4.426534982071949\n -3.1068165552293254\n -1.0820421327607177\n  3.1495967624505226","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"Using that in this case, the intervals are bracketing intervals for f, we can find the zeros from the roots output with the following:","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"julia> [find_zero(f, (interval(u).lo, interval(u).hi)) for u ∈ rts if u.status == :unique]\n4-element Vector{Float64}:\n  3.1495967624505226\n -4.426534982071949\n -3.1068165552293254\n -1.082042132760718","category":"page"},{"location":"roots/#Adding-a-solver","page":"An overview of Roots","title":"Adding a solver","text":"","category":"section"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"To add a solver the minimum needed is a type to declare the solver and an update_state method. In this example, we also define a state object, as the algorithm, as employed, uses more values stored than the default.","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"The Wikipedia page for Brent's method suggest a modern improvement, Chandrapatla's method, described here. The algorithm there is mostly followed below. This is from the implementation of Roots.Chandrapatla.","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"To implement Chandrapatla's algorithm we first define a type to indicate the method and a state object which records the values x_n, x_n-1, and x_n-2, needed for the inverse quadratic step.","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"julia> struct Chandrapatla <: Roots.AbstractAcceleratedBisection end\n\njulia> struct ChandrapatlaState{T,S} <: Roots.AbstractUnivariateZeroState{T,S}\n    xn1::T\n    xn0::T\n    c::T\n    fxn1::S\n    fxn0::S\n    fc::S\nend\n","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"An init_state method can be used by some methods to add more detail to the basic state object. Here it starts the old value, c off as a as a means to ensure an initial bisection step.","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"julia> function init_state(::Chandrapatla, F, x₀, x₁, fx₀, fx₁)\n    a, b, fa, fb = x₁, x₀, fx₁, fx₀\n    c, fc = a, fa\n    ChandrapatlaState(b, a, c, fb, fa, fc)\nend","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"The main algorithm is implemented in the update_state method. The @set! macro from Setfield.jl is used to modify a state object, which otherwise is immutable.","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"julia> import Roots.Setfield: @set!;\n\njulia> function Roots.update_state(::Chandrapatla, F, o, options, l=NullTracks())\n\n    b, a, c = o.xn1, o.xn0, o.c\n    fb, fa, fc = o.fxn1, o.fxn0, o.fc\n\n    # encoding: a = xₙ, b = xₙ₋₁, c = xₙ₋₂\n    ξ = (a - b) / (c - b)\n    ϕ = (fa - fb) / (fc - fb)\n    ϕ² = ϕ^2\n    Δ = (ϕ² < ξ) && (1 - 2ϕ + ϕ² < 1 - ξ) # Chandrapatla's inequality to determine next step\n\n    xₜ = Δ ? Roots.inverse_quadratic_step(a, b, c, fa, fb, fc) : a + (b-a)/2\n\n    fₜ = F(xₜ)\n    incfn(l)\n\n    if sign(fₜ) * sign(fa) < 0\n        a, b, c = xₜ, a, b\n        fa, fb, fc = fₜ, fa, fb\n    else\n        a, c = xₜ, a\n        fa, fc = fₜ, fa\n    end\n\n    @set! o.xn0 = a\n    @set! o.xn1 = b\n    @set! o.c = c\n    @set! o.fxn0 = fa\n    @set! o.fxn1 = fb\n    @set! o.fc = fc\n\n    return (o, false)\nend\n","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"This algorithm chooses between an inverse quadratic step or a bisection step depending on the relationship between the computed ξ and Φ. The tolerances are from the default for AbstractBracketingMethod.","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"To see that the algorithm works, we have:","category":"page"},{"location":"roots/","page":"An overview of Roots","title":"An overview of Roots","text":"julia> find_zero(sin, (3,4), Chandrapatla())\n3.1415926535897927\n\njulia> find_zero(x -> exp(x) - x^4, (8,9), Chandrapatla())\n8.613169456441398\n\njulia> find_zero(x -> x^5 - x - 1, (1,2), Chandrapatla())\n1.1673039782614185","category":"page"},{"location":"reference/#Reference/API","page":"Reference/API","title":"Reference/API","text":"","category":"section"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"The Roots  package provides several  different  algorithms  to solve f(x)=0.","category":"page"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"Pages = [\"reference.md\"]","category":"page"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"using Roots","category":"page"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"DocTestSetup = quote\n  using Roots\nend","category":"page"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"CurrentModule = Roots","category":"page"},{"location":"reference/#The-find_zero-and-find_zeros-functions","page":"Reference/API","title":"The find_zero  and find_zeros functions","text":"","category":"section"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"There are  two main  functions:  find_zero   to  identify  a  zero  of  f  given  some initial starting  value or  bracketing interval and  find_zeros to heuristically identify  all  zeros in  a specified interval.","category":"page"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"find_zero\nfind_zeros","category":"page"},{"location":"reference/#Roots.find_zero","page":"Reference/API","title":"Roots.find_zero","text":"find_zero(f, x0, M, [N::AbstractBracketingMethod]; kwargs...)\n\nInterface to one of several methods for finding zeros of a univariate function, e.g. solving f(x)=0.\n\nArguments\n\nPositional arguments\n\nf: the function (univariate or f(x,p) with p holding parameters)\nx0: the initial condition (a value, initial values, or bracketing interval)\nM: some AbstractUnivariateZeroMethod specifying the solver\nN: some bracketing method, when specified creates a hybrid method\n\nKeyword arguments\n\nxatol, xrtol: absolute and relatative tolerance to decide if xₙ₊₁ ≈ xₙ\natol, rtol: absolute and relatative tolerance to decide if f(xₙ) ≈ 0\nmaxiters: specify the maximum number of iterations the algorithm can take.\nverbose::Bool: specifies if details about algorithm should be shown\ntracks: allows specification of Tracks objecs\n\nInitial starting value\n\nFor most methods, x0 is a scalar value indicating the initial value in the iterative procedure. (Secant methods can have a tuple specify their initial values.) Values must be a subtype of Number and have methods for float, real, and oneunit defined.\n\nFor bracketing intervals, x0 is specified using a tuple, a vector, or any iterable with extrema defined. A bracketing interval, ab, is one where f(a) and f(b) have different signs.\n\nReturn value\n\nIf the algorithm suceeds, the approximate root identified is returned. A ConvergenceFailed error is thrown if the algorithm fails. The alternate form solve(ZeroProblem(f,x0), M) returns NaN in case of failure.\n\nSpecifying a method\n\nA method is specified to indicate which algorithm to employ:\n\nThere are methods where a bracket is specified: Bisection, A42, AlefeldPotraShi, Roots.Brent, among others. Bisection is the default for basic floating point types, butA42` generally requires far fewer iterations.\nThere are several derivative-free methods: cf. Order0, Order1 (also Roots.Secant), Order2 (also Steffensen), Order5, Order8, and Order16, where the number indicates the order of the convergence. Methods Roots.Order1B and Roots.Order2B are useful when the desired zero has a multiplicity.\nThere are some classical methods where derivatives are required: Roots.Newton, Roots.Halley, Roots.Schroder, among others.\nThe family Roots.LithBoonkkampIJzerman{S,D} ,for different S and D, uses a linear multistep method root finder. The (2,0) method is the secant method, (1,1) is Newton's method.\n\nFor more detail, see the help page for each method (e.g., ?Order1). Non-exported methods must be qualified with module name, as in ?Roots.Schroder.\n\nIf no method is specified, the default method depends on x0:\n\nIf x0 is a scalar, the default is the more robust Order0 method.\nIf x0 is a tuple, vector, or iterable with extrema defined indicating a bracketing interval, then the Bisection method is used for Float64, Float32 or Float16 types; otherwise the A42 method is used.\n\nThe default methods are chosen to be robust; they may not be as efficient as some others.\n\nSpecifying the function\n\nThe function(s) are passed as the first argument.\n\nFor the few methods that use one or more derivatives (Newton, Halley, Schroder, LithBoonkkampIJzerman(S,D), etc.) a tuple of functions is used. For the classical algorithms, a function returning (f(x), f(x)/f'(x), [f'(x)/f''(x)]) may be used.\n\nOptional arguments (tolerances, limit evaluations, tracing)\n\nxatol - absolute tolerance for x values.\nxrtol - relative tolerance for x values.\natol  - absolute tolerance for f(x) values.\nrtol  - relative tolerance for f(x) values.\nmaxiters   - limit on maximum number of iterations.\nstrict - if false (the default), when the algorithm stops, possible zeros are checked with a relaxed tolerance.\nverbose - if true a trace of the algorithm will be shown on successful completion. See the internal Tracks object to save this trace.\n\nSee the help string for Roots.assess_convergence for details on convergence. See the help page for Roots.default_tolerances(method) for details on the default tolerances.\n\nIn general, with floating point numbers, convergence must be understood as not an absolute statement. Even if mathematically α is an answer and xstar the floating point realization, it may be that f(xstar) - f(α)  ≈ xstar ⋅  f'(α) ⋅ eps(α), so the role of tolerances must be appreciated, and at times specified.\n\nFor the Bisection methods, convergence is guaranteed over Float64 values, so the tolerances are set to be 0 by default.\n\nIf a bracketing method is passed in after the method specification, then whenever a bracket is identified during the algorithm, the method will switch to the bracketing method to identify the zero. (Bracketing methods are mathematically guaranteed to converge, non-bracketing methods may or may not converge.)  This is what Order0 does by default, with an initial secant method switching to the AlefeldPotraShi method should a bracket be encountered.\n\nNote: The order of the method is hinted at in the naming scheme. A scheme is order r if, with eᵢ = xᵢ - α, eᵢ₊₁ = C⋅eᵢʳ. If the error eᵢ is small enough, then essentially the error will gain r times as many leading zeros each step. However, if the error is not small, this will not be the case. Without good initial guesses, a high order method may still converge slowly, if at all. The OrderN methods have some heuristics employed to ensure a wider range for convergence at the cost of not faithfully implementing the method, though those are available through unexported methods.\n\nExamples:\n\nDefault methods.\n\njulia> using Roots\n\njulia> find_zero(sin, 3)  # use Order0()\n3.141592653589793\n\njulia> find_zero(sin, (3,4)) # use Bisection()\n3.141592653589793\n\nSpecifying a method,\n\njulia> find_zero(sin, (3,4), Order1())            # can specify two starting points for secant method\n3.141592653589793\n\njulia> find_zero(sin, 3.0, Order2())              # Use Steffensen method\n3.1415926535897936\n\njulia> find_zero(sin, big(3.0), Order16())        # rapid convergence\n3.141592653589793238462643383279502884197169399375105820974944592307816406286198\n\njulia> find_zero(sin, (3, 4), A42())              # fewer function calls than Bisection(), in this case\n3.141592653589793\n\njulia> find_zero(sin, (3, 4), FalsePosition(8))   # 1 of 12 possible algorithms for false position\n3.141592653589793\n\njulia> find_zero((sin,cos), 3.0, Roots.Newton())  # use Newton's method\n3.141592653589793\n\njulia> find_zero((sin, cos, x->-sin(x)), 3.0, Roots.Halley())  # use Halley's method\n3.141592653589793\n\nChanging tolerances.\n\njulia> fn = x -> (2x*cos(x) + x^2 - 3)^10/(x^2 + 1);\n\njulia> x0, xstar = 3.0,  2.9947567209477;\n\njulia> fn(find_zero(fn, x0, Order2())) <= 1e-14  # f(xₙ) ≈ 0, but Δxₙ can be largish\ntrue\n\njulia> find_zero(fn, x0, Order2(), atol=0.0, rtol=0.0) # error: x_n ≉ x_{n-1}; just f(x_n) ≈ 0\nERROR: Roots.ConvergenceFailed(\"Algorithm failed to converge\")\n[...]\n\njulia> fn = x -> (sin(x)*cos(x) - x^3 + 1)^9;\n\njulia> x0, xstar = 1.0,  1.112243913023029;\n\njulia> find_zero(fn, x0, Order2()) ≈ xstar\ntrue\n\njulia> find_zero(fn, x0, Order2(), maxiters=3)    # need more steps to converge\nERROR: Roots.ConvergenceFailed(\"Algorithm failed to converge\")\n[...]\n\nTracing\n\nPassing verbose=true will show details on the steps of the algorithm. The tracks argument allows the passing of a Tracks object to record the values of x and f(x) used in the algorithm.\n\nnote: Note\nSee solve! and ZeroProblem for an alternate interface.\n\n\n\n\n\n","category":"function"},{"location":"reference/#Roots.find_zeros","page":"Reference/API","title":"Roots.find_zeros","text":"find_zeros(f, a, b=nothing; [no_pts=12, k=8, naive=false, xatol, xrtol, atol, rtol])\n\nSearch for zeros of f in the interval [a,b]. This interval can be specified with two values or using a single value, such as a tuple or vector, for which extrema returns two distinct values in increasing order.\n\nExamples\n\njulia> using Roots\n\njulia> find_zeros(x -> exp(x) - x^4, -5, 20)        # a few well-spaced zeros\n3-element Vector{Float64}:\n -0.8155534188089606\n  1.4296118247255556\n  8.613169456441398\n\njulia> find_zeros(x -> sin(x^2) + cos(x)^2, 0, 2pi)  # many zeros\n12-element Vector{Float64}:\n 1.78518032659534\n 2.391345462376604\n 3.2852368649448853\n 3.3625557095737544\n 4.016412952618305\n 4.325091924521049\n 4.68952781386834\n 5.00494459113514\n 5.35145266881871\n 5.552319796014526\n 5.974560835055425\n 6.039177477770888\n\njulia> find_zeros(x -> cos(x) + cos(2x), (0, 4pi))    # mix of simple, non-simple zeros\n6-element Vector{Float64}:\n  1.0471975511965976\n  3.141592653589943\n  5.235987755982988\n  7.330382858376184\n  9.424777960769228\n 11.519173063162574\n\njulia> f(x) = (x-0.5) * (x-0.5001) * (x-1)          # nearby zeros\nf (generic function with 1 method)\n\njulia> find_zeros(f, 0, 2)\n3-element Vector{Float64}:\n 0.5\n 0.5001\n 1.0\n\njulia> f(x) = (x-0.5) * (x-0.5001) * (x-4) * (x-4.001) * (x-4.2)\nf (generic function with 1 method)\n\njulia> find_zeros(f, 0, 10)\n3-element Vector{Float64}:\n 0.5\n 0.5001\n 4.2\n\njulia> f(x) = (x-0.5)^2 * (x-0.5001)^3 * (x-4) * (x-4.001) * (x-4.2)^2  # hard to identify\nf (generic function with 1 method)\n\njulia> find_zeros(f, 0, 10, no_pts=21)                # too hard for default\n5-element Vector{Float64}:\n 0.49999999999999994\n 0.5001\n 4.0\n 4.001\n 4.200000000000001\n\nnote: Note\nThere are two cases where the number of zeros may be underreported:if the initial interval, (a,b), is too wide\nif there are zeros  that are very nearby\n\n\n\nThe basic algorithm checks for zeros among the endpoints, and then divides the interval (a,b) into no_pts-1 subintervals and then proceeds to look for zeros through bisection or a derivative-free method.  As checking for a bracketing interval is relatively cheap and bisection is guaranteed to converge, each interval has k pairs of intermediate points checked for a bracket.\n\nIf any zeros are found, the algorithm uses these to partition (a,b) into subintervals. Each subinterval is shrunk so that the endpoints are not zeros and the process is repeated on the subinterval. If the initial interval is too large, then the naive scanning for zeros may be fruitless and no zeros will be reported. If there are nearby zeros, the shrinking of the interval may jump over them, though as seen in the examples, nearby roots can be identified correctly, though for really nearby points, or very flat functions, it may help to increase no_pts.\n\nThe tolerances are used to shrink the intervals, but not to find zeros within a search. For searches, bisection is guaranteed to converge with no specified tolerance. For the derivative free search, a modification of the Order0 method is used, which at worst case compares |f(x)| <= 8*eps(x) to identify a zero. The algorithm might identify more than one value for a zero, due to floating point approximations. If a potential pair of zeros satisfy isapprox(a,b,atol=sqrt(xatol), rtol=sqrt(xrtol)) then they are consolidated.\n\nThe algorithm can make many function calls. When zeros are found in an interval, the naive search is carried out on each subinterval. To cut down on function calls, though with some increased chance of missing some zeros, the adaptive nature can be skipped with the argument naive=true or the number of points stepped down.\n\nThe algorithm is derived from one in a PR by @djsegal.\n\nnote: Note\nThe IntervalRootFinding package provides a rigorous alternative to this heuristic one. That package uses interval arithmetic, so can compute bounds on the size of the image of an interval under f. If this image includes 0, then it can look for the zero. Bisection, on the other hand, only will look for a zero if the two endpoints have different signs, a much more rigid condition for a potential zero.\n\nFor example, this function (due to @truculentmath) is particularly tricky, as it is positive at every floating point number, but has two zeros (the vertical asymptote at 15//11 is only negative within adjacent floating point values):\n\njulia> using IntervalArithmetic, IntervalRootFinding, Roots\n\njulia> g(x) = x^2 + 1 +log(abs( 11*x-15 ))/99\ng (generic function with 1 method)\n\njulia> find_zeros(g, -3, 3)\nFloat64[]\n\njulia> IntervalRootFinding.roots(g, -3..3, IntervalRootFinding.Bisection)\n1-element Vector{Root{Interval{Float64}}}:\n Root([1.36363, 1.36364], :unknown)\n\nA less extreme usage might be the following, where unique indicates Bisection could be useful and indeed find_zeros will identify these values:\n\njulia> g(x) = exp(x) - x^5\ng (generic function with 1 method)\n\njulia> rts = IntervalRootFinding.roots(g, -20..20)\n2-element Vector{Root{Interval{Float64}}}:\n Root([12.7132, 12.7133], :unique)\n Root([1.29585, 1.29586], :unique)\n\njulia> find_zeros(g, -20, 20)\n2-element Vector{Float64}:\n  1.2958555090953687\n 12.713206788867632\n\n\n\n\n\n","category":"function"},{"location":"reference/#CommonSolve-interface","page":"Reference/API","title":"CommonSolve interface","text":"","category":"section"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"The problem-algorithm-solve interface is a pattern popularized in Julia by the DifferentialEquations.jl suite of packages. This can be used as an alternative to find_zero. Unlike find_zero, solve will return NaN on non-convergence.","category":"page"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"Roots.solve!\nRoots.ZeroProblem","category":"page"},{"location":"reference/#CommonSolve.solve!","page":"Reference/API","title":"CommonSolve.solve!","text":"solve(fx::ZeroProblem, [M], [N]; p=nothing, kwargs...)\ninit(fx::ZeroProblem, [M], [N];\n     p=nothing,\n     verbose=false, tracks=NullTracks(), kwargs...)\nsolve!(P::ZeroProblemIterator)\n\nSolve for the zero of a scalar-valued univariate function specified through ZeroProblem or ZeroProblemIterator using the CommonSolve interface.\n\nThe defaults for M and N depend on the ZeroProblem: if x0 is a number, then M=Secant() and N=AlefeldPotraShi() is used (Order0); if x0 has 2 (or more values) then it is assumed to be a bracketing interval and M=AlefeldPotraShi() is used.\n\nThe methods involved with this interface are:\n\nZeroProblem: used to specify a problem with a function (or functions) and an initial guess\nsolve: to solve for a zero in a ZeroProblem\n\nThe latter calls the following, which can be useful independently:\n\ninit: to initialize an iterator with a method for solution, any adjustments to the default tolerances, and a specification to log the steps or not.\nsolve! to iterate to convergence.\n\nReturns NaN, not an error like find_zero, when the problem can not be solved. Tested for zero allocations.\n\nExamples:\n\njulia> using Roots\n\njulia> fx = ZeroProblem(sin, 3)\nZeroProblem{typeof(sin), Int64}(sin, 3)\n\njulia> solve(fx)\n3.141592653589793\n\nOr, if the iterable is required\n\njulia> problem = init(fx);\n\njulia> solve!(problem)\n3.141592653589793\n\nkeyword arguments can be used to adjust the default tolerances.\n\njulia> solve(fx, Order5(); atol=1/100)\n3.1425464815525403\n\nThe above is equivalent to:\n\njulia> problem = init(fx, Order5(), atol=1/100);\n\njulia> solve!(problem)\n3.1425464815525403\n\nThe keyword argument p may be used if the function(s) to be solved depend on a parameter in their second positional argument (e.g., f(x, p)). For example\n\njulia> f(x,p) = exp(-x) - p # to solve p = exp(-x)\nf (generic function with 1 method)\n\njulia> fx = ZeroProblem(f, 1)\nZeroProblem{typeof(f), Int64}(f, 1)\n\njulia> solve(fx; p=1/2)  # log(2)\n0.6931471805599453\n\nThis would be recommended, as there is no recompilation due to the function changing.\n\nThe argument verbose=true for init instructs that steps to be logged;\n\nThe iterator interface allows for the creation of hybrid solutions, such as is used when two methods are passed to solve. For example, this is essentially how the hybrid default is constructed:\n\njulia> function order0(f, x)\n           fx = ZeroProblem(f, x)\n           p = init(fx, Roots.Secant())\n           xᵢ,st = ϕ = iterate(p)\n           while ϕ !== nothing\n               xᵢ, st = ϕ\n               state, ctr = st\n               fᵢ₋₁, fᵢ = state.fxn0, state.fxn1\n               if sign(fᵢ₋₁)*sign(fᵢ) < 0 # check for bracket\n                   x0 = (state.xn0, state.xn1)\n                   fx′ = ZeroProblem(f, x0)\n                   p = init(fx′, Bisection())\n                   xᵢ = solve!(p)\n                   break\n               end\n               ϕ = iterate(p, st)\n           end\n           xᵢ\n       end\norder0 (generic function with 1 method)\n\njulia> order0(sin, 3)\n3.141592653589793\n\n\n\n\n\n","category":"function"},{"location":"reference/#Roots.ZeroProblem","page":"Reference/API","title":"Roots.ZeroProblem","text":"ZeroProblem{F,X}\n\nA container for a function and initial guess to be used with solve.\n\n\n\n\n\n","category":"type"},{"location":"reference/#Classical-methods-based-on-derivatives","page":"Reference/API","title":"Classical  methods  based on derivatives","text":"","category":"section"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"We begin  by  describing  the classical methods even though they are not necessarily  recommended  because they require more work of the  user,  as they give insight into  why there  are a variety  of methods available.","category":"page"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"The classical  methods of Newton and  Halley utilize information about  the function  and  its derivative(s) in  an  iterative manner  to converge to  a zero of  f(x) given an initial starting value.","category":"page"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"Newton's method is   easily described:","category":"page"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"From  an initial point,  the  next  point  in  the iterative algorithm is found by identifying the  intersection of  the x    axis  with  the tangent line of f at the initial  point. This is repeated until convergence  or the realization that   convergence won't happen for the  initial point. Mathematically,","category":"page"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"x_n+1  =  x_n  - f(x_n)f(x_n)","category":"page"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"Some facts  are helpful  to  understand the different   methods  available in Roots:","category":"page"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"For Newton's method there is a formula for the error: Set epsilon_n = alpha - x_n, where alpha is the zero, then epsilon_n+1 = -f(xi_n)(2f(xi_n) cdot epsilon_n^2 here xi_n is some value between alpha and x_n.\nThe error term, when of the form epsilon_n+1 leq Ccdotepsilon_n^2, can be used to identify an interval around alpha for which convergence is guaranteed. Such convergence is termed quadratic (order 2).  For floating point solutions, quadratic convergence and a well chosen initial point can lead to convergence in 4 or 5 iterations. In general, convergence is termed order q when epsilon_n+1 approx Ccdotepsilon_n^q\nThe term -f(xi_n)(2f(xi_n) indicates possible issues  when f  is  too big  near alpha  or  f is too small  near alpha. In particular if f(alpha)  =  0, there need  not be quadratic  convergence, and convergence  can   take many  iterations. A  zero   for which f(x) = (x-alpha)^1+betacdot g(x), with g(alpha) neq 0  is called simple when beta=0 and  non-simple when  beta   0. Newton's method is quadratic near simple  zeros and need not be quadratic  near  non-simple zeros.","category":"page"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"As well,  if  f is too  big near alpha, or  f too small near  alpha, or x_n  too  far  from  alpha (that is,  epsilon_n1) the  error  might actually increase and convergence is not guaranteed.","category":"page"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"The explicit form of  the error function can  be used to guarantee convergence for functions with a certain shape (monotonic, convex functions where the sign of f and f don't change). Quadratic convergence may only occur once the algorithm is near the zero.\nThe number of function evaluations  per step for Newton's method is 2.","category":"page"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"","category":"page"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"Roots.Newton\nRoots.Halley\nRoots.QuadraticInverse\nRoots.ChebyshevLike\nRoots.SuperHalley","category":"page"},{"location":"reference/#Roots.Newton","page":"Reference/API","title":"Roots.Newton","text":"Roots.Newton()\n\nImplements Newton's method: xᵢ₊₁ =  xᵢ - f(xᵢ)/f'(xᵢ).  This is a quadratically convergent method requiring one derivative. Two function calls per step.\n\nExamples\n\njulia> using Roots\n\njulia> find_zero((sin,cos), 3.0, Roots.Newton()) ≈ π\ntrue\n\nIf function evaluations are expensive one can pass in a function which returns (f, f/f') as follows\n\njulia> find_zero(x -> (sin(x), sin(x)/cos(x)), 3.0, Roots.Newton()) ≈ π\ntrue\n\nThis can be advantageous if the derivative is easily computed from the value of f, but otherwise would be expensive to compute.\n\n\n\nThe error, eᵢ = xᵢ - α, can be expressed as eᵢ₊₁ = f[xᵢ,xᵢ,α]/(2f[xᵢ,xᵢ])eᵢ² (Sidi, Unified treatment of regula falsi, Newton-Raphson, secant, and Steffensen methods for nonlinear equations).\n\n\n\n\n\n","category":"type"},{"location":"reference/#Roots.Halley","page":"Reference/API","title":"Roots.Halley","text":"Roots.Halley()\n\nImplements Halley's method, xᵢ₊₁ = xᵢ - (f/f')(xᵢ) * (1 - (f/f')(xᵢ) * (f''/f')(xᵢ) * 1/2)^(-1) This method is cubically converging, it requires 3 function calls per step.\n\nThe function, its derivative and second derivative can be passed either as a tuple of 3 functions or as a function returning values for (f ff ff), which could be useful when function evaluations are expensive.\n\nExamples\n\njulia> using Roots\n\njulia> find_zero((sin, cos, x->-sin(x)), 3.0, Roots.Halley()) ≈ π\ntrue\n\njulia> function f(x)\n       s,c = sincos(x)\n       (s, s/c, -c/s)\n       end;\n\njulia> find_zero(f, 3.0, Roots.Halley()) ≈ π\ntrue\n\nThis can be advantageous if the derivatives are easily computed from the computation for f, but otherwise would be expensive to compute separately.\n\n\n\nThe error, eᵢ = xᵢ - α, satisfies eᵢ₊₁ ≈ -(2f'⋅f''' -3⋅(f'')²)/(12⋅(f'')²) ⋅ eᵢ³ (all evaluated at α).\n\n\n\n\n\n","category":"type"},{"location":"reference/#Roots.QuadraticInverse","page":"Reference/API","title":"Roots.QuadraticInverse","text":"Roots.QuadraticInverse()\n\nImplements the quadratic inverse method also known as Chebyshev's method, xᵢ₊₁ = xᵢ - (f/f')(xᵢ) * (1 + (f/f')(xᵢ) * (f''/f')(xᵢ) * 1/2). This method is cubically converging, it requires 3 function calls per step.\n\nExample\n\njulia> using Roots\n\njulia> find_zero((sin, cos, x->-sin(x)), 3.0, Roots.QuadraticInverse()) ≈ π\ntrue\n\nIf function evaluations are expensive one can pass in a function which returns (f, f/f',f'/f'') as follows\n\njulia> find_zero(x -> (sin(x), sin(x)/cos(x), -cos(x)/sin(x)), 3.0, Roots.QuadraticInverse()) ≈ π\ntrue\n\nThis can be advantageous if the derivatives are easily computed from the computation for f, but otherwise would be expensive to compute separately.\n\nThe error, eᵢ = xᵢ - α, satisfies eᵢ₊₁ ≈ (1/2⋅(f''/f')² - 1/6⋅f'''/f')) ⋅ eᵢ³ (all evaluated at α).\n\n\n\n\n\n","category":"type"},{"location":"reference/#Roots.ChebyshevLike","page":"Reference/API","title":"Roots.ChebyshevLike","text":"CHEBYSHEV-LIKE METHODS AND QUADRATIC EQUATIONS (J. A. EZQUERRO, J. M. GUTIÉRREZ, M. A. HERNÁNDEZ and M. A. SALANOVA)\n\n\n\n\n\n","category":"type"},{"location":"reference/#Roots.SuperHalley","page":"Reference/API","title":"Roots.SuperHalley","text":"An acceleration of Newton's method: Super-Halley method (J.M. Gutierrez, M.A. Hernandez\n\n\n\n\n\n","category":"type"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"Newton and Halley's method are members of this family of methods:","category":"page"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"Roots.LithBoonkkampIJzerman{S,D}","category":"page"},{"location":"reference/#Roots.LithBoonkkampIJzerman","page":"Reference/API","title":"Roots.LithBoonkkampIJzerman","text":"LithBoonkkampIJzerman{S,D} <: AbstractNewtonLikeMethod\nLithBoonkkampIJzerman(S,D)\n\nA family of different methods that includes the secant method and Newton's method.\n\nSpecifies a linear multistep solver with S steps and D derivatives following Lith, Boonkkamp, and IJzerman.\n\nExamples\n\njulia> using Roots\n\njulia> find_zero(sin, 3, Roots.LithBoonkkampIJzerman(2,0)) ≈ π # the secant method\ntrue\n\njulia> find_zero((sin,cos), 3, Roots.LithBoonkkampIJzerman(1,1)) ≈ π # Newton's method\ntrue\n\njulia> find_zero((sin,cos), 3, Roots.LithBoonkkampIJzerman(3,1)) ≈ π # Faster convergence rate\ntrue\n\njulia> find_zero((sin,cos, x->-sin(x)), 3, Roots.LithBoonkkampIJzerman(1,2)) ≈ π # Halley-like method\ntrue\n\nThe method can be more robust to the intial condition. This example is from the paper (p13). Newton's method (the S=1, D=1 case) fails if |x₀| ≥ 1.089 but methods with more memory succeed.\n\njulia> fx =  ZeroProblem((tanh,x->sech(x)^2), 1.239); # zero at 0.0\n\njulia> solve(fx, Roots.LithBoonkkampIJzerman(1,1)) |> isnan# Newton, NaN\ntrue\n\njulia> solve(fx, Roots.LithBoonkkampIJzerman(2,1)) |> abs |> <(eps())\ntrue\n\njulia> solve(fx, Roots.LithBoonkkampIJzerman(3,1)) |> abs |> <(eps())\ntrue\n\nMultiple derivatives can be constructed automatically using automatic differentiation. For example,\n\njulia> using ForwardDiff\n\njulia> function δ(f, n::Int=1)\n           n <= 0 && return f\n           n == 1 && return x -> ForwardDiff.derivative(f,float(x))\n           δ(δ(f,1),n-1)\n       end;\n\njulia> fs(f,n) = ntuple(i -> δ(f,i-1), Val(n+1));\n\njulia> f(x) = cbrt(x) * exp(-x^2); # cf. Table 6 in paper, α = 0\n\njulia> fx = ZeroProblem(fs(f,1), 0.1147);\n\njulia> opts = (xatol=2eps(), xrtol=0.0, atol=0.0, rtol=0.0); # converge if |xₙ - xₙ₋₁| <= 2ϵ\n\njulia> solve(fx, Roots.LithBoonkkampIJzerman(1, 1); opts...) |> isnan # NaN -- no convergence\ntrue\n\njulia> solve(fx, Roots.LithBoonkkampIJzerman(2, 1); opts...) |> abs |> <(eps()) # converges\ntrue\n\njulia> fx = ZeroProblem(fs(f,2), 0.06);                       # need better starting point\n\njulia> solve(fx, Roots.LithBoonkkampIJzerman(2, 2); opts...) |> abs |> <(eps()) # converges\ntrue\n\nFor the case D=1, a bracketing method based on this approach is implemented in LithBoonkkampIJzermanBracket\n\nReference\n\nIn Lith, Boonkkamp, and IJzerman an analysis is given of the convergence rates when using linear multistep methods to solve 0=f(x) using f⁻¹(0) = x when f is a sufficiently smooth linear function. The reformulation, attributed to Grau-Sanchez, finds a differential equation for f⁻¹: dx/dy = [f⁻¹]′(y) = 1/f′(x) = F as x(0) = x₀ + ∫⁰_y₀ F(x(y)) dy.\n\nA linear multi-step method is used to solve this equation numerically.  Let S be the number of memory steps (S= 1,2,...) and D be the number of derivatives employed, then, with F(x) = dx/dy x_{n+S} = ∑_{k=0}^{S-1} aₖ x_{n+k} +∑d=1^D ∑_{k=1}^{S-1} aᵈ_{n+k}F⁽ᵈ⁾(x_{n+k}).  The aₖs and aᵈₖs are computed each step.\n\nThis table is from Tables 1 and 3 of the paper and gives the convergence rate for simple roots identified therein:\n\ns: number of steps remembered\nd: number of derivatives uses\ns/d  0    1    2    3    4\n1    .    2    3    4    5\n2    1.62 2.73 3.79 4.82 5.85\n3    1.84 2.91 3.95 4.97 5.98\n4    1.92 2.97 3.99 4.99 5.996\n5    1.97 .    .    .    .\n\nThat is, more memory leads to a higher convergence rate; more derivatives leads to a higher convergence rate. However, the interval about α, the zero, where the convergence rate is guaranteed may get smaller.\n\nnote: Note\nFor the larger values of S, the expressions to compute the next value get quite involved. The higher convergence rate is likely only to be of help for finding solutions to high precision.\n\n\n\n\n\n","category":"type"},{"location":"reference/#Derivative-free-methods","page":"Reference/API","title":"Derivative free methods","text":"","category":"section"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"The secant method replaces the  derivative term in Newton's method with the slope of the secant line.","category":"page"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"x_n+1 = x_n - (fracf(x_n)-f(x_n-1)x_n - x_n-1)^-1cdot  f(x_n)","category":"page"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"Though the secant  method   has  convergence  rate of  order approx 1618 and is not quadratic,  it only requires one new  function call per  step  so  can be very effective. Often  function evaluations are the  slowest part of  the computation and, as  well, no derivative is  needed. Because  it  can be  very efficient, the secant  method  is used in  the default method  of find_zero when  called with a single initial starting point.","category":"page"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"Steffensen's method is a quadratically converging. derivative-free method  which uses a secant  line  based on x_n and x_n + f(x_n).  Though of  higher  order, it requires  additional function calls per step and depends on a  good initial starting value. Other  derivative free methods are available, trading off  increased function calls for higher-order convergence. They may be  of interest when arbitrary  precision is needed. A  measure of efficiency is q^1r where q is the order of convergence and r the number of function calls per step.   With this measure, the secant method  would be approx (1618)^11 and Steffensen's  would be less (2^12).","category":"page"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"","category":"page"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"Secant\nSteffensen\nOrder5\nOrder8\nOrder16","category":"page"},{"location":"reference/#Roots.Secant","page":"Reference/API","title":"Roots.Secant","text":"Secant()\nOrder1()\nOrderφ()\n\nThe Order1() method is an alias for Secant. It specifies the secant method. This method keeps two values in its state, xₙ and xₙ₋₁. The updated point is the intersection point of x axis with the secant line formed from the two points. The secant method uses 1 function evaluation per step and has order φ≈ (1+sqrt(5))/2.\n\nThe error, eᵢ = xᵢ - α, satisfies eᵢ₊₂ = f[xᵢ₊₁,xᵢ,α] / f[xᵢ₊₁,xᵢ] * (xᵢ₊₁-α) * (xᵢ - α).\n\n\n\n\n\n","category":"type"},{"location":"reference/#Roots.Steffensen","page":"Reference/API","title":"Roots.Steffensen","text":"Steffensen()\nOrder2()\n\nThe quadratically converging Steffensen method is used for the derivative-free Order2() algorithm. Unlike the quadratically converging Newton's method, no derivative is necessary, though like Newton's method, two function calls per step are. Steffensen's algorithm is more sensitive than Newton's method to poor initial guesses when f(x) is large, due to how f'(x) is approximated. The Order2 method replaces a Steffensen step with a secant step when f(x) is large.\n\nThe error, eᵢ - α, satisfies eᵢ₊₁ = f[xᵢ, xᵢ+fᵢ, α] / f[xᵢ,xᵢ+fᵢ] ⋅ (1 - f[xᵢ,α] ⋅ eᵢ²\n\n\n\n\n\n","category":"type"},{"location":"reference/#Roots.Order5","page":"Reference/API","title":"Roots.Order5","text":"Order5()\nKumarSinghAkanksha()\n\nImplements an order 5 algorithm from A New Fifth Order Derivative Free Newton-Type Method for Solving Nonlinear Equations by Manoj Kumar, Akhilesh Kumar Singh, and Akanksha, Appl. Math. Inf. Sci. 9, No. 3, 1507-1513 (2015), DOI: 10.12785/amis/090346. Four function calls per step are needed.  The Order5 method replaces a Steffensen step with a secant step when f(x) is large.\n\nThe error, eᵢ = xᵢ - α, satisfies eᵢ₊₁ = K₁ ⋅ K₅ ⋅ M ⋅ eᵢ⁵ + O(eᵢ⁶)\n\n\n\n\n\n","category":"type"},{"location":"reference/#Roots.Order8","page":"Reference/API","title":"Roots.Order8","text":"Order8()\nThukral8()\n\nImplements an eighth-order algorithm from New Eighth-Order Derivative-Free Methods for Solving Nonlinear Equations by Rajinder Thukral, International Journal of Mathematics and Mathematical Sciences Volume 2012 (2012), Article ID 493456, 12 pages DOI: 10.1155/2012/493456. Four function calls per step are required.  The Order8 method replaces a Steffensen step with a secant step when f(x) is large.\n\nThe error, eᵢ = xᵢ - α, is expressed as eᵢ₊₁ = K ⋅ eᵢ⁸ in (2.25) of the paper for an explicit K.\n\n\n\n\n\n","category":"type"},{"location":"reference/#Roots.Order16","page":"Reference/API","title":"Roots.Order16","text":"Order16()\nThukral16()\n\nImplements the order 16 algorithm from New Sixteenth-Order Derivative-Free Methods for Solving Nonlinear Equations by R. Thukral, American Journal of Computational and Applied Mathematics p-ISSN: 2165-8935;    e-ISSN: 2165-8943; 2012;  2(3): 112-118 DOI: 10.5923/j.ajcam.20120203.08.\n\nFive function calls per step are required. Though rapidly converging, this method generally isn't faster (fewer function calls/steps) over other methods when using Float64 values, but may be useful for solving over BigFloat.  The Order16 method replaces a Steffensen step with a secant step when f(x) is large.\n\nThe error, eᵢ = xᵢ - α, is expressed as eᵢ₊₁ = K⋅eᵢ¹⁶ for an explicit K in equation (50) of the paper.\n\n\n\n\n\n","category":"type"},{"location":"reference/#Bracketing-methods","page":"Reference/API","title":"Bracketing methods","text":"","category":"section"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"The bisection identifies a zero of a continuous function between a and b  when  f(a) and  f(b) have different  signs. (The interval ab is called a bracketing interval when f(a)cdot  f(b)  0.)  The basic  algorithm is particularly simple, an interval  a_ib_i is  split  at  c =  (a_i+b_i)2. Either  f(c)=0,  or one  of  a_ic  or  cb_i is a bracketing  interval,  which is  called  a_i+1b_i+1. From this  description,  we  see  that  a_ib_i has length  2^-i times the length of a_0b_0, so  the intervals will eventually terminate by finding  a zero, c,  or converge  to a zero. This convergence is slow (the efficiency  is only 1, but guaranteed. For  16-, 32-, and 64-bit  floating point  values, a  reinterpretation  of  how the midpoint  (c) is found  leads  to convergence  in  no more  than   64 iterations, unlike the midpoint found above, where some cases can take many more steps to converge.","category":"page"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"In floating point,  by  guaranteed  convergence we have either an exact zero or a bracketing interval  consisting   of  two  adjacent floating point values. When applied to non-continuous  functions,  this algorithm  will identify   an exact  zero or  a zero crossing   of the function. (E.g., applied  to  f(x)=1x it  will  find  0.)","category":"page"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"The default selection of  midpoint described above includes no information  about the function f bounds its  sign. Algorithms exploiting  the shape of the function  can be significantly more efficient. For example, the bracketing method Roots.AlefeldPotraShi due to Alefeld, Potra, and Shi has  efficiency approx 16686. This method  is  also   used in the  default method for find_zero when a  single initial starting point is given if a bracketing interval is identified.","category":"page"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"","category":"page"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"Bisection\nRoots.A42\nRoots.AlefeldPotraShi\nRoots.Brent\nRoots.Chandrapatla\nRoots.Ridders\nRoots.ITP\nFalsePosition\nRoots.LithBoonkkampIJzermanBracket","category":"page"},{"location":"reference/#Roots.Bisection","page":"Reference/API","title":"Roots.Bisection","text":"Bisection()\n\nIf possible, will use the bisection method over Float64 values. The bisection method starts with a bracketing interval [a,b] and splits it into two intervals [a,c] and [c,b], If c is not a zero, then one of these two will be a bracketing interval and the process continues. The computation of c is done by _middle, which reinterprets floating point values as unsigned integers and splits there. It was contributed  by  Jason Merrill. This method avoids floating point issues and when the tolerances are set to zero (the default) guarantees a \"best\" solution (one where a zero is found or the bracketing interval is of the type [a, nextfloat(a)]).\n\nWhen tolerances are given, this algorithm terminates when the interval length is less than or equal to the tolerance max(δₐ, 2abs(u)δᵣ) with u in {a,b} chosen by the smaller of |f(a)| and |f(b)|, or  or the function value is less than max(tol, min(abs(a), abs(b)) * rtol). The latter is used only if the default tolerances (atol or rtol) are adjusted.\n\n\n\n\n\n","category":"type"},{"location":"reference/#Roots.A42","page":"Reference/API","title":"Roots.A42","text":"Roots.A42()\n\nBracketing method which finds the root of a continuous function within a provided interval [a, b], without requiring derivatives. It is based on algorithm 4.2 described in: G. E. Alefeld, F. A. Potra, and Y. Shi, \"Algorithm 748: enclosing zeros of continuous functions,\" ACM Trans. Math. Softw. 21, 327–344 (1995), DOI: 10.1145/210089.210111. The asymptotic efficiency index, q^1k, is (2 + 7^12)^13 = 16686.\n\nOriginally by John Travers.\n\n\n\n\n\n","category":"type"},{"location":"reference/#Roots.AlefeldPotraShi","page":"Reference/API","title":"Roots.AlefeldPotraShi","text":"Roots.AlefeldPotraShi()\n\nFollows algorithm in \"ON ENCLOSING SIMPLE ROOTS OF NONLINEAR EQUATIONS\", by Alefeld, Potra, Shi; DOI: 10.1090/S0025-5718-1993-1192965-2.\n\nThe order of convergence is 2 + √5; asymptotically there are 3 function evaluations per step. Asymptotic efficiency index is (2+5)^(13)  1618. Less efficient, but can run faster than the A42 method.\n\nOriginally by John Travers.\n\n\n\n\n\n","category":"type"},{"location":"reference/#Roots.Brent","page":"Reference/API","title":"Roots.Brent","text":"Roots.Brent()\n\nAn implementation of Brent's (or Brent-Dekker) method. This method uses a choice of inverse quadratic interpolation or a secant step, falling back on bisection if necessary.\n\n\n\n\n\n","category":"type"},{"location":"reference/#Roots.Chandrapatla","page":"Reference/API","title":"Roots.Chandrapatla","text":"Roots.Chandrapatla()\n\nUse Chandrapatla's algorithm (cf. Scherer to solve f(x) = 0.\n\nChandrapatla's algorithm chooses between an inverse quadratic step or a bisection step based on a computed inequality.\n\n\n\n\n\n","category":"type"},{"location":"reference/#Roots.Ridders","page":"Reference/API","title":"Roots.Ridders","text":"Roots.Ridders()\n\nImplements Ridders' method. This bracketing method finds the midpoint, x₁; then interpolates an exponential; then uses false position with the interpolated value to find c. If c and x₁ form a bracket is used, otherwise the subinterval [a,c] or [c,b] is used.\n\nExample:\n\njulia> using Roots\n\njulia> find_zero(x -> exp(x) - x^4, (5, 15), Roots.Ridders()) ≈ 8.61316945644\ntrue\n\njulia> find_zero(x -> x*exp(x) - 10, (-100, 100), Roots.Ridders()) ≈ 1.74552800274\ntrue\n\njulia> find_zero(x -> tan(x)^tan(x) - 1e3, (0, 1.5), Roots.Ridders()) ≈ 1.3547104419\ntrue\n\nRidders showed the error satisfies eₙ₊₁ ≈ 1/2 eₙeₙ₋₁eₙ₋₂ ⋅ (g^2-2fh)/f for f=F', g=F''/2, h=F'''/6, suggesting converence at rate ≈ 1.839.... It uses two function evaluations per step, so  its order of convergence is ≈ 1.225....\n\n\n\n\n\n","category":"type"},{"location":"reference/#Roots.ITP","page":"Reference/API","title":"Roots.ITP","text":"Roots.ITP(;[κ₁-0.2, κ₂=2, n₀=1])\n\nUse the ITP bracketing method.  This method claims it \"is the first root-finding algorithm that achieves the superlinear convergence of the secant method while retaining the optimal worst-case performance of the bisection method.\"\n\nThe values κ1, κ₂, and n₀ are tuning parameters.\n\nThe suggested value of κ₁ is 0.2/(b-a), but the default here is 0.2. The value of κ₂ is 2, and the default value of n₀ is 1.\n\nNote:\n\nSuggested on discourse by @TheLateKronos, who supplied the original version of the code below.\n\n\n\n\n\n","category":"type"},{"location":"reference/#Roots.FalsePosition","page":"Reference/API","title":"Roots.FalsePosition","text":"FalsePosition([galadino_factor])\n\nUse the false position method to find a zero for the function f within the bracketing interval [a,b].\n\nThe false position method is a modified bisection method, where the midpoint between [aₖ, bₖ] is chosen to be the intersection point of the secant line with the x axis, and not the average between the two values.\n\nTo speed up convergence for concave functions, this algorithm implements the 12 reduction factors of Galdino (A family of regula falsi root-finding methods). These are specified by number, as in FalsePosition(2) or by one of three names FalsePosition(:pegasus), FalsePosition(:illinois), or FalsePosition(:anderson_bjork) (the default). The default choice has generally better performance than the others, though there are exceptions.\n\nFor some problems, the number of function calls can be greater than for the Bisection method, but generally this algorithm will make fewer function calls.\n\nExamples\n\nfind_zero(x -> x^5 - x - 1, (-2, 2), FalsePosition())\n\n\n\n\n\n","category":"type"},{"location":"reference/#Roots.LithBoonkkampIJzermanBracket","page":"Reference/API","title":"Roots.LithBoonkkampIJzermanBracket","text":"LithBoonkkampIJzermanBracket()\n\nA bracketing method which is a modification of Brent's method due to Lith, Boonkkamp, and IJzerman. The best possible convergence rate is 2.91.\n\nA function, its derivative, and a bracketing interval need to be specified.\n\nThe state includes the 3 points – a bracket [a,b] (b=xₙ has f(b) closest to 0) and c=xₙ₋₁ – and the corresponding values for the function and its derivative at these three points.\n\nThe next proposed step is either a S=2 or S=3 selection for the LithBoonkkampIJzerman methods with derivative information included only if it would be of help. The proposed is modified if it is dithering. The proposed is compared against a bisection step; the one in the bracket and with the smaller function value is chosen as the next step.\n\n\n\n\n\n","category":"type"},{"location":"reference/#Non-simple-zeros","page":"Reference/API","title":"Non-simple zeros","text":"","category":"section"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"The order of convergence for most methods is for simple zeros, values alpha where f(x) = (x-alpha) cdot g(x), with g(alpha) being non-zero. For methods which are of order k for non-simple zeros, usually an additional function call is needed per step. For example, this is the case for Roots.Newton as compared to Roots.Schroder.","category":"page"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"Derivative-free methods for non-simple zeros have the following implemented:","category":"page"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"Roots.King\nRoots.Esser","category":"page"},{"location":"reference/#Roots.King","page":"Reference/API","title":"Roots.King","text":"Roots.Order1B()\nRoots.King()\n\nA superlinear (order 1.6...) modification of the secant method for multiple roots. Presented in A SECANT METHOD FOR MULTIPLE ROOTS, by RICHARD F. KING, BIT 17 (1977), 321-328\n\nThe basic idea is similar to Schroder's method: apply the secant method to  f/f'. However, this uses f' ~ fp = (fx - f(x-fx))/fx (a Steffensen step). In this implementation, Order1B, when fx is too big, a single secant step of f is used.\n\nThe asymptotic error, eᵢ = xᵢ - α, is given by eᵢ₊₂ = 1/2⋅G''/G'⋅ eᵢ⋅eᵢ₊₁ + (1/6⋅G'''/G' - (1/2⋅G''/G'))^2⋅eᵢ⋅eᵢ₊₁⋅(eᵢ+eᵢ₊₁).\n\n\n\n\n\n","category":"type"},{"location":"reference/#Roots.Esser","page":"Reference/API","title":"Roots.Esser","text":"Roots.Order2B()\nRoots.Esser()\n\nEsser's method. This is a quadratically convergent method that, like Schroder's method, does not depend on the multiplicity of the zero. Schroder's method has update step x - r2/(r2-r1) * r1, where ri = fⁱ⁻¹/fⁱ. Esser approximates f' ~ f[x-h, x+h], f'' ~ f[x-h,x,x+h], where h = fx, as with Steffensen's method, Requiring 3 function calls per step. The implementation Order2B uses a secant step when |fx| is considered too large.\n\nEsser, H. Computing (1975) 14: 367. DOI: 10.1007/BF02253547 Eine stets quadratisch konvergente Modifikation des Steffensen-Verfahrens\n\nExamples\n\nf(x) = cos(x) - x\ng(x) = f(x)^2\nx0 = pi/4\nfind_zero(f, x0, Order2(), verbose=true)        #  3 steps / 7 function calls\nfind_zero(f, x0, Roots.Order2B(), verbose=true) #  4 / 9\nfind_zero(g, x0, Order2(), verbose=true)        #  22 / 45\nfind_zero(g, x0, Roots.Order2B(), verbose=true) #  4 / 10\n\n\n\n\n\n","category":"type"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"For non-simple zeros, Schroder showed an additional derivative can  be used to yield quadratic convergence based on Newton's method:","category":"page"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"Roots.Schroder","category":"page"},{"location":"reference/#Roots.Schroder","page":"Reference/API","title":"Roots.Schroder","text":"Roots.Schroder()\n\nSchröder's method, like Halley's method, utilizes f, f', and f''. Unlike Halley it is quadratically converging, but this is independent of the multiplicity of the zero (cf. Schröder, E. \"Über unendlich viele Algorithmen zur Auflösung der Gleichungen.\" Math. Ann. 2, 317-365, 1870; mathworld).\n\nSchröder's method applies Newton's method to f/f', a function with all simple zeros.\n\nExample\n\nm = 2\nf(x) = (cos(x)-x)^m\nfp(x) = (-x + cos(x))*(-2*sin(x) - 2)\nfpp(x) = 2*((x - cos(x))*cos(x) + (sin(x) + 1)^2)\nfind_zero((f, fp, fpp), pi/4, Roots.Halley())     # 14 steps\nfind_zero((f, fp, fpp), 1.0, Roots.Schroder())    # 3 steps\n\n(Whereas, when m=1, Halley is 2 steps to Schröder's 3.)\n\nIf function evaluations are expensive one can pass in a function which returns (f, f/f',f'/f'') as follows\n\nfind_zero(x -> (sin(x), sin(x)/cos(x), -cos(x)/sin(x)), 3.0, Roots.Schroder())\n\nThis can be advantageous if the derivatives are easily computed from the value of f, but otherwise would be expensive to compute.\n\nThe error, eᵢ = xᵢ - α, is the same as Newton with f replaced by f/f'.\n\n\n\n\n\n","category":"type"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"A family of methods for non-simple zeros which require k derivatives to be order k, with k=2 yielding Schroder's method, are implemented in:","category":"page"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"Roots.AbstractThukralBMethod","category":"page"},{"location":"reference/#Roots.AbstractThukralBMethod","page":"Reference/API","title":"Roots.AbstractThukralBMethod","text":"AbstractThukralBMethod\n\nAbstract type for ThukralXB methods for X being 2,3,4, or 5.\n\nThese are a family of methods which are\n\nefficient (order X) for non-simple roots (e.g. Thukral2B is the Schroder method)\ntake X+1 function calls per step\nrequire X derivatives. These can be passed as a tuple of functions, (f, f', f'', …), or as\n\na function returning the ratios: x -> (f(x), f(x)/f'(x), f'(x)/f''(x), …).\n\nExamples\n\nusing ForwardDiff\nBase.adjoint(f::Function)  = x  -> ForwardDiff.derivative(f, float(x))\nf(x) = (exp(x) + x - 2)^6\nx0 = 1/4\nfind_zero((f, f', f''), x0, Roots.Halley())               # 14 iterations; ≈ 48 function evaluations\nfind_zero((f, f', f''), big(x0), Roots.Thukral2B())       #  3 iterations; ≈ 9 function evaluations\nfind_zero((f, f', f'', f'''), big(x0), Roots.Thukral3B()) #  2 iterations; ≈ 8 function evaluations\n\nRefrence\n\nIntroduction to a family of Thukral k-order method for finding multiple zeros of nonlinear equations, R. Thukral, JOURNAL OF ADVANCES IN MATHEMATICS 13(3):7230-7237, DOI: 10.24297/jam.v13i3.6146.\n\n\n\n\n\n","category":"type"},{"location":"reference/#Hybrid-methods","page":"Reference/API","title":"Hybrid  methods","text":"","category":"section"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"A useful  strategy  is   to  begin with a non-bracketing  method and switch to a bracketing method should a bracket be encountered. This  allows   for the identification of zeros which are not surrounded by a bracket, and have guaranteed convergence  should a bracket be  encountered.  It  is  used  by default by find_zero(f,a).","category":"page"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"Roots.Order0","category":"page"},{"location":"reference/#Roots.Order0","page":"Reference/API","title":"Roots.Order0","text":"Order0()\n\nThe Order0 method is engineered to be a more robust, though possibly slower, alternative to the other derivative-free root-finding methods. The implementation roughly follows the algorithm described in Personal Calculator Has Key to Solve Any Equation f(x) = 0, the SOLVE button from the HP-34C. The basic idea is to use a secant step. If along the way a bracket is found, switch to a bracketing algorithm, using AlefeldPotraShi.  If the secant step fails to decrease the function value, a quadratic step is used up to 4 times.\n\nThis is not really 0-order: the secant method has order 16 Wikipedia and the the bracketing method has order 16180 Wikipedia so for reasonable starting points and functions, this algorithm should be superlinear, and relatively robust to non-reasonable starting points.\n\n\n\n\n\n","category":"type"},{"location":"reference/#Rates-of-convergence","page":"Reference/API","title":"Rates of convergence","text":"","category":"section"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"The order of a method is q, where e_i+1 approx e_i^q. Newton's method is famously quadratic for simple roots; the secant method of order q approx varphi=1618dots. However, p=2 calls are needed for Newton's method, and only p=1 for the secant method. The asymptotic efficiency is q^1p, which penalizes function calls. There are other order k methods taking k function calls per step, e.g., Halley's; others take fewer, as seen below. Many use inverse quadratic steps, others inverse cubic–these have order q solving q^s+1-2q^s+1 (s=3 for quadratic). For robust methods, generally 1 additional function call is needed to achieve the convergence rate, Schroder being a good example.","category":"page"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"Type Method Order F evals Asymptotic efficiency\nHybrid Order0   approx 1618dots\nDerivative Free Secant varphi=1618dots 1 1618dots\nDerivative Free Steffensen 2 2 1414dots\nDerivative Free Order5 5 4 1495dots\nDerivative Free Order8 8 4 1681dots\nDerivative Free Order16 16 5 1718dots\nClassical Newton 2 2 1414dots\nClassical Halley 3 3 1442dots\nClassical QuadraticInverse 3 3 1442dots\nClassical ChebyshevLike 3 3 1442dots\nClassical SuperHalley 3 3 1442dots\nMultiStep LithBoonkkampIJzerman{S,D} p^s=sum p^k(d+sigma_k) D+1 varies, 192dots max\nBracketing BisectionExact 1 1 1\nBracketing A42 (2 + 7^12) 34 (2 + 7^12)^13 = 16686dots\nBracketing AlefeldPotraShi  34 1618dots\nBracketing Brent leq 189dots 1 leq 189dots\nBracketing ITP ``\\leq \\varphi 1 leq varphi\nBracketing Ridders 183dots 2 1225dots\nBracketing FalsePosition 1442dots 1 1442dots\nBracketing LithBoonkkampIJzermanBracket 291 3 1427dots\nRobust King varphi=1618dots 2 1272dots\nRobust Esser 2 3 1259dots\nRobust Schroder 2 3 1259dots\nRobust Thukral3 3 4 1316dots\nRobust Thukral4 4 5 1319dots\nRobust Thukral5 5 6 1307dots","category":"page"},{"location":"reference/#Convergence","page":"Reference/API","title":"Convergence","text":"","category":"section"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"Identifying when an algorithm converges or diverges requires specifications of tolerances  and convergence criteria.","category":"page"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"In the case of exact bisection, convergence is mathematically guaranteed. For floating point numbers, either an exact zero is found, or the bracketing interval can be subdivided into a_nb_n with a_n and b_n being adjacent floating point values. That is b_n-a_n is as small as possible in floating point numbers. This can be considered a stopping criteria in Delta x. For early termination (less precision but fewer function calls) a tolerance can be given so that if Delta_n=b_n-a_n is small enough the algorithm stops successfully.  In floating point, assessing if b_n approx a_n requires two tolerances: a relative tolerance, as the minimal differences in floating point values depend on the size of b_n and a_n, and an absolute tolerance for values near 0. The values xrtol and xatol are passed to the Base.isapprox function to determine closeness.","category":"page"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"Relying on the closeness of two x values will not be adequate for all problems, as there are examples where the difference Delta_n=x_n-x_n-1 can be quite small, 0 even, yet f(x_n) is not near a 0. As such, for non-bracketing methods, a check on the size of f(x_n) is also used. As we find floating point approximations to alpha, the zero, we must consider values small when f(alpha(1+epsilon)) is small. By Taylor's approximation, we can expect this to be around alphacdot epsilon cdot f(alpha). That is, small depends on the size of alpha and the derivative at alpha.  The former is handled by both relative and absolute tolerances (rtol and atol).  The size of f(alpha) is problem dependent, and can be accommodated by larger relative or absolute tolerances.","category":"page"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"When an algorithm returns  a  NaN value,  it terminates. This  can  happen near convergence or  may indicate some issues.  Early termination is checked for convergence  in the  size  of f(x_n) with a relaxed tolerance when strict=false is specified (the default).","category":"page"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"note: Relative tolerances  and assessing  `f(x) ≈ 0`\nThe use of  relative tolerances  to  check  if   f(x)  approx  0 can lead  to spurious  answers  where  x is very large   (and  hence the relative  tolerance  is large). The return of  very  large solutions  should  be checked against expectations  of the  answer.","category":"page"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"Deciding if an algorithm won't  terminate is  done  through  counting the number or  iterations performed; the default  adjusted through maxiters. As  most  algorithms are superlinear, convergence happens rapidly near  the answer, but  all the algorithms  can take  a while  to  get near  an  answer, even   when progress  is made. As  such, the maximum must be large enough to consider linear cases, yet small enough to avoid too many steps when an algorithm is non-convergent.","category":"page"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"Convergence criteria are method dependent and are determined by  the  Roots.assess_convergence  methods.","category":"page"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"Roots.assess_convergence","category":"page"},{"location":"reference/#Roots.assess_convergence","page":"Reference/API","title":"Roots.assess_convergence","text":"Roots.assess_convergence(method, state, options)\n\nAssess if algorithm has converged.\n\nReturn a convergence flag and a Boolean indicating if algorithm has terminated (converged or not converged)\n\nIf algrithm hasn't converged this returns (:not_converged, false).\n\nIf algorithm has stopped or converged, return flag and true. Flags are:\n\n:x_converged if xn1 ≈ xn, typically with non-zero tolerances specified.\n:f_converged if  |f(xn1)| < max(atol, |xn1|*rtol)\n:nan or :inf if fxn1 is NaN or an infinity.\n:not_converged if algorithm should continue\n\nDoes not check number of steps taken nor number of function evaluations.\n\nIn decide_convergence, stopped values (and :x_converged when strict=false) are checked for convergence with a relaxed tolerance.\n\n\n\n\n\n","category":"function"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"Default tolerances  are specified through the Roots.default_tolerances methods.","category":"page"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"Roots.default_tolerances","category":"page"},{"location":"reference/#Roots.default_tolerances","page":"Reference/API","title":"Roots.default_tolerances","text":"default_tolerances(M::AbstractUnivariateZeroMethod, [T], [S])\n\nThe default tolerances for most methods are xatol=eps(T), xrtol=eps(T), atol=4eps(S), and rtol=4eps(S), with the proper units (absolute tolerances have the units of x and f(x); relative tolerances are unitless). For Complex{T} values, T is used.\n\nThe number of iterations is limited by maxiters=40.\n\n\n\n\n\ndefault_tolerances(M::AbstractBisectionMethod, [T], [S])\n\nFor Bisection when the x values are of type Float64, Float32, or Float16, the default tolerances are zero and there is no limit on the number of iterations. In this case, the algorithm is guaranteed to converge to an exact zero, or a point where the function changes sign at one of the answer's adjacent floating point values.\n\nFor other types, default non-zero tolerances for xatol and xrtol are given.\n\n\n\n\n\n","category":"function"},{"location":"reference/#Simplified-versions","page":"Reference/API","title":"Simplified versions","text":"","category":"section"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"The abstractions and many checks for  convergence employed by find_zero have a performance cost. When that is a critical concern, there are  several \"simple\" methods provided which can offer improved performance.","category":"page"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"Roots.secant_method\nRoots.bisection\nRoots.muller\nRoots.newton\nRoots.dfree","category":"page"},{"location":"reference/#Roots.secant_method","page":"Reference/API","title":"Roots.secant_method","text":"secant_method(f, xs; [atol=0.0, rtol=8eps(), maxevals=1000])\n\nPerform secant method to solve f(x) = 0.\n\nThe secant method is an iterative method with update step given by b - fb/m where m is the slope of the secant line between (a,fa) and (b,fb).\n\nThe inital values can be specified as a pair of 2, as in (x₀, x₁) or [x₀, x₁], or as a single value, x₁ in which case a value of x₀ is chosen.\n\nThe algorithm returns m when abs(fm) <= max(atol, abs(m) * rtol). If this doesn't occur before maxevals steps or the algorithm encounters an issue, a value of NaN is returned. If too many steps are taken, the current value is checked to see if there is a sign change for neighboring floating point values.\n\nThe Order1 method for find_zero also implements the secant method. This one should be slightly faster, as there are fewer setup costs.\n\nExamples:\n\nRoots.secant_method(sin, (3,4))\nRoots.secant_method(x -> x^5 -x - 1, 1.1)\n\nnote: Specialization\nThis function will specialize on the function f, so that the inital call can take more time than a call to the Order1() method, though subsequent calls will be much faster.  Using FunctionWrappers.jl can ensure that the initial call is also equally as fast as subsequent ones.\n\n\n\n\n\n","category":"function"},{"location":"reference/#Roots.bisection","page":"Reference/API","title":"Roots.bisection","text":"bisection(f, a, b; [xatol, xrtol])\n\nPerforms bisection method to find a zero of a continuous function.\n\nIt is assumed that (a,b) is a bracket, that is, the function has different signs at a and b. The interval (a,b) is converted to floating point and shrunk when a or b is infinite. The function f may be infinite for the typical case. If f is not continuous, the algorithm may find jumping points over the x axis, not just zeros.\n\nIf non-trivial tolerances are specified, the process will terminate when the bracket (a,b) satisfies isapprox(a, b, atol=xatol, rtol=xrtol). For zero tolerances, the default, for Float64, Float32, or Float16 values, the process will terminate at a value x with f(x)=0 or f(x)*f(prevfloat(x)) < 0 or f(x) * f(nextfloat(x)) < 0. For other number types, the Roots.A42 method is used.\n\n\n\n\n\n","category":"function"},{"location":"reference/#Roots.muller","page":"Reference/API","title":"Roots.muller","text":"muller(f, xᵢ; xatol=nothing, xrtol=nothing, maxevals=100)\nmuller(f, xᵢ₋₂, xᵢ₋₁, xᵢ; xatol=nothing, xrtol=nothing, maxevals=100)\n\nMuller’s method generalizes the secant method, but uses quadratic interpolation among three points instead of linear interpolation between two. Solving for the zeros of the quadratic allows the method to find complex pairs of roots. Given three previous guesses for the root xᵢ₋₂, xᵢ₋₁, xᵢ, and the values of the polynomial f at those points, the next approximation xᵢ₊₁ is produced.\n\nExcerpt and the algorithm taken from\n\nW.H. Press, S.A. Teukolsky, W.T. Vetterling and B.P. Flannery Numerical Recipes in C, Cambridge University Press (2002), p. 371\n\nConvergence here is decided by xᵢ₊₁ ≈ xᵢ using the tolerances specified, which both default to eps(one(typeof(abs(xᵢ))))^4/5 in the appropriate units. Each iteration performs three evaluations of f. The first method picks two remaining points at random in relative proximity of xᵢ.\n\nNote that the method may return complex result even for real intial values as this depends on the function.\n\nExamples:\n\nmuller(x->x^3-1, 0.5, 0.5im, -0.5) # → -0.500 + 0.866…im\nmuller(x->x^2+2, 0.0, 0.5, 1.0) # → ≈ 0.00 - 1.41…im\nmuller(x->(x-5)*x*(x+5), rand(3)...) # → ≈ 0.00\nmuller(x->x^3-1, 1.5, 1.0, 2.0) # → 2.0, Not converged\n\n\n\n\n\n","category":"function"},{"location":"reference/#Roots.newton","page":"Reference/API","title":"Roots.newton","text":"Roots.newton(f, fp, x0; kwargs...)\n\nImplementation of Newton's method: xᵢ₊₁ =  xᵢ - f(xᵢ)/f'(xᵢ).\n\nArguments:\n\nf::Function – function to find zero of\nfp::Function – the derivative of f.\nx0::Number – initial guess. For Newton's method this may be complex.\n\nWith the FowardDiff package derivatives may be computed automatically. For example,  defining D(f) = x -> ForwardDiff.derivative(f, float(x)) allows D(f) to be used for the first derivative.\n\nKeyword arguments are passed to find_zero using the Roots.Newton() method.\n\nSee also Roots.newton((f,fp), x0) and Roots.newton(fΔf, x0) for simpler implementations.\n\n\n\n\n\n","category":"function"},{"location":"reference/#Roots.dfree","page":"Reference/API","title":"Roots.dfree","text":"dfree(f, xs)\n\nA more robust secant method implementation\n\nSolve for f(x) = 0 using an alogorithm from Personal Calculator Has Key to Solve Any Equation f(x) = 0, the SOLVE button from the HP-34C.\n\nThis is also implemented as the Order0 method for find_zero.\n\nThe inital values can be specified as a pair of two values, as in (a,b) or [a,b], or as a single value, in which case a value of b is computed, possibly from fb.  The basic idea is to follow the secant method to convergence unless:\n\na bracket is found, in which case AlefeldPotraShi is used;\nthe secant method is not converging, in which case a few steps of a quadratic method are used to see if that improves matters.\n\nConvergence occurs when f(m) == 0, there is a sign change between m and an adjacent floating point value, or f(m) <= 2^3*eps(m).\n\nA value of NaN is returned if the algorithm takes too many steps before identifying a zero.\n\nExamples\n\nRoots.dfree(x -> x^5 - x - 1, 1.0)\n\n\n\n\n\n","category":"function"},{"location":"reference/#MATLAB-interface","page":"Reference/API","title":"MATLAB interface","text":"","category":"section"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"The initial naming scheme used fzero instead  of fzeros, following the name of the  MATLAB function fzero. This interface  is not recommended, but, for now, still maintained.","category":"page"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"Roots.fzero","category":"page"},{"location":"reference/#Roots.fzero","page":"Reference/API","title":"Roots.fzero","text":"fzero(f, x0; order=0; kwargs...)\nfzero(f, x0, M; kwargs...)\nfzero(f, x0, M, N; kwargs...)\nfzero(f, x0; kwargs...)\nfzero(f, a::Number, b::Number; kwargs...)\nfzero(f, a::Number, b::Number; order=?, kwargs...)\nfzero(f, fp, a::Number; kwargs...)\n\nFind zero of a function using one of several iterative algorithms.\n\nf: a scalar function or callable object\nx0: an initial guess, a scalar value or tuple of two values\norder: An integer, symbol, or string indicating the algorithm to  use for find_zero. The Order0 default may be specified directly  by order=0, order=:0, or order=\"0\"; Order1() by order=1,  order=:1, order=\"1\", or order=:secant; Order1B() by  order=\"1B\", etc.\nM: a specific method, as would be passed to find_zero, bypassing the use of the order keyword\nN: a specific bracketing method. When given, if a bracket is identified, method N will be used to finish instead of method M.\na, b: When two values are passed along, if no order value is specified, Bisection will be used over the bracketing interval (a,b). If an order value is specified, the value of x0 will be set to (a,b) and the specified method will be used.\nfp: when fp is specified (assumed to compute the derivative of f), Newton's method will be used\nkwargs...: See find_zero for the specification of tolerances and other keyword arguments\n\nExamples:\n\nfzero(sin, 3)                  # use Order0() method, the default\nfzero(sin, 3, order=:secant)   # use secant method (also just `order=1`)\nfzero(sin, 3, Roots.Order1B()) # use secant method variant for multiple roots.\nfzero(sin, 3, 4)               # use bisection method over (3,4)\nfzero(sin, 3, 4, xatol=1e-6)   # use bisection method until |x_n - x_{n-1}| <= 1e-6\nfzero(sin, 3, 3.1, order=1)    # use secant method with x_0=3.0, x_1 = 3.1\nfzero(sin, (3, 3.1), order=2)  # use Steffensen's method with x_0=3.0, x_1 = 3.1\nfzero(sin, cos, 3)             # use Newton's method\n\nnote: Note\nUnlike find_zero, fzero does not specialize on the type of the function argument. This has the advantage of making the first use of the function f faster, but subsequent uses slower.\n\n\n\n\n\n","category":"function"},{"location":"reference/#Tracking-iterations","page":"Reference/API","title":"Tracking iterations","text":"","category":"section"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"It is possible to add the keyword arguement verbose=true to when calling the find_zero function to get detailed information about the solution, and data from each iteration. If you want to save this data instead of just printing it, you can use a Tracks object.","category":"page"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"","category":"page"},{"location":"reference/","page":"Reference/API","title":"Reference/API","text":"Roots.Tracks","category":"page"},{"location":"reference/#Roots.Tracks","page":"Reference/API","title":"Roots.Tracks","text":"Tracks(T, S)\nTracks()\n\nConstruct a Tracks object used to record the progress of the algorithm. T is the type of function inputs, and S is the type of function outputs. They both default to Float64. Note that because this type is not exported, you have to write Roots.Tracks() to construct a Tracks object.\n\nBy default, no tracking is done while finding a root. To change this, construct a Tracks object, and pass it to the keyword argument tracks. This will modify the Tracks object, storing the input and function values at each iteration, along with additional information about the root-finding process.\n\nTracks objects are shown in an easy-to-read format. Internally either a tuple of (x,f(x)) pairs or (aₙ, bₙ) pairs are stored, the latter for bracketing methods. (These implementation details may change without notice.) The methods empty!, to reset the Tracks object; get, to get the tracks; last, to get the value convered to, may be of interest.\n\nIf you only want to print the information, but you don't need it later, this can conveniently be done by passing verbose=true to the root-finding function. This will not effect the return value, which will still be the root of the function.\n\nExamples\n\njulia> using Roots\n\njulia> f(x) = x^2-2\nf (generic function with 1 method)\n\njulia> tracker = Roots.Tracks()\nAlgorithm has not been run\n\njulia> find_zero(f, (0, 2), Roots.Secant(), tracks=tracker) ≈ √2\ntrue\n\njulia> tracker\nResults of univariate zero finding:\n\n* Converged to: 1.4142135623730947\n* Algorithm: Secant()\n* iterations: 7\n* function evaluations ≈ 9\n* stopped as |f(x_n)| ≤ max(δ, |x|⋅ϵ) using δ = atol, ϵ = rtol\n\nTrace:\nx₁ = 0,\t fx₁ = -2\nx₂ = 2,\t fx₂ = 2\nx₃ = 1,\t fx₃ = -1\nx₄ = 1.3333333333333333,\t fx₄ = -0.22222222222222232\nx₅ = 1.4285714285714286,\t fx₅ = 0.04081632653061229\nx₆ = 1.4137931034482758,\t fx₆ = -0.0011890606420930094\nx₇ = 1.4142114384748701,\t fx₇ = -6.0072868388605372e-06\nx₈ = 1.4142135626888697,\t fx₈ = 8.9314555751229818e-10\nx₉ = 1.4142135623730947,\t fx₉ = -8.8817841970012523e-16\n\njulia> empty!(tracker)  # resets\n\njulia> find_zero(sin, (3, 4), Roots.A42(), tracks=tracker) ≈ π\ntrue\n\njulia> get(tracker)\n4-element Vector{NamedTuple{names, Tuple{Float64, Float64}} where names}:\n (a = 3.0, b = 4.0)\n (a = 3.0, b = 3.5)\n (a = 3.14156188905231, b = 3.1416247553172956)\n (a = 3.141592653589793, b = 3.1415926535897936)\n\njulia> last(tracker)\n3.141592653589793\n\nnote: Note\nAs designed, the Tracks object may not record actions taken while the state object is initialized. An example is the default bisection algorithm where an initial midpoint is found to ensure the bracket does not straddle 0.\n\n\n\n\n\n","category":"type"},{"location":"#Roots.jl","page":"Roots.jl","title":"Roots.jl","text":"","category":"section"},{"location":"","page":"Roots.jl","title":"Roots.jl","text":"Documentation for Roots.jl","category":"page"},{"location":"#About","page":"Roots.jl","title":"About","text":"","category":"section"},{"location":"","page":"Roots.jl","title":"Roots.jl","text":"Roots is  a Julia package  for finding zeros of continuous scalar functions of a single real variable. That  is solving f(x)=0 for x. The find_zero function provides the primary interface. It supports various algorithms through the specification of a method. These include:","category":"page"},{"location":"","page":"Roots.jl","title":"Roots.jl","text":"Bisection-like algorithms. For functions where a bracketing interval is known (one where f(a) and f(b) have alternate signs), there are several bracketing methods, including Bisection.  For most floating point number types, bisection occurs in a manner exploiting floating point storage conventions leading to an exact zero or a bracketing interval as small as floating point computations allows. Other methods include Roots.A42, Roots.AlefeldPotraShi, Roots.Brent, Roots.Chandrapatlu, Roots.ITP, Roots.Ridders, and 12-flavors of FalsePosition. The default bracketing method is Bisection for the basic floating-point types, as it is more robust to some inputs, but A42 and AlefeldPotraShi typically converge in a few iterations and are more performant.","category":"page"},{"location":"","page":"Roots.jl","title":"Roots.jl","text":"Several derivative-free methods are implemented. These are specified through the methods Order0, Order1 (the secant method), Order2 (the Steffensen method), Order5, Order8, and Order16. The number indicates, roughly, the order of convergence. The Order0 method is the default, and the most robust, as it finishes off with a bracketing method when a bracket is encountered, The higher order methods promise higher order (faster) convergence, though don't always yield results with fewer function calls than Order1 or Order2. The methods Roots.Order1B and Roots.Order2B are superlinear and quadratically converging methods independent of the multiplicity of the zero.","category":"page"},{"location":"","page":"Roots.jl","title":"Roots.jl","text":"There are methods that require a derivative or two: Roots.Newton, Roots.Halley are classical ones, Roots.QuadraticInverse, Roots.ChebyshevLike, Roots.SuperHaller are others. Roots.Schroder provides a quadratic method, like Newton's method, which is independent of the multiplicity of the zero. The Roots.ThukralXB, X=2, 3, 4, or 5 are also multiplicity three. The X denotes the number of derivatives that need specifying. The Roots.LithBoonkkampIJzerman{S,D} methods remember S steps and use D derivatives.","category":"page"},{"location":"#Basic-usage","page":"Roots.jl","title":"Basic usage","text":"","category":"section"},{"location":"","page":"Roots.jl","title":"Roots.jl","text":"Consider  the polynomial   function  f(x) = x^5 - x + 12. As a polynomial,  its roots, or  zeros, could  be identified with the  roots function of  the Polynomials package. However, even  that function uses a numeric method to identify   the values, as no  solution with radicals is available. That is, even for polynomials, non-linear root finders are needed to solve f(x)=0.","category":"page"},{"location":"","page":"Roots.jl","title":"Roots.jl","text":"The Roots package provides a variety of algorithms for this  task. In this overview, only the  default ones  are illustrated.","category":"page"},{"location":"","page":"Roots.jl","title":"Roots.jl","text":"For  the function f(x) = x^5 - x + 12 a simple plot will show a zero  somewhere between -12 and -10 and two zeros near 06.","category":"page"},{"location":"","page":"Roots.jl","title":"Roots.jl","text":"For the zero between two values at which the function changes sign, a bracketing method is useful, as bracketing methods are guaranteed to converge for continuous functions by the intermediate value theorem. A bracketing algorithm will be used when the initial data is passed as a tuple:","category":"page"},{"location":"","page":"Roots.jl","title":"Roots.jl","text":"julia> using Roots\n\njulia> f(x) =  x^5 - x + 1/2\nf (generic function with 1 method)\n\njulia> find_zero(f, (-1.2,  -1)) ≈ -1.0983313019186336\ntrue","category":"page"},{"location":"","page":"Roots.jl","title":"Roots.jl","text":"The default algorithm is guaranteed to have an  answer nearly as accurate as is  possible  given the limitations of floating point  computations.","category":"page"},{"location":"","page":"Roots.jl","title":"Roots.jl","text":"For the zeros \"near\" a point,  a non-bracketing method is often used, as generally  the algorithms are more efficient and can be  used in cases where a zero does  not. Passing just  the initial point will dispatch to  such a method:","category":"page"},{"location":"","page":"Roots.jl","title":"Roots.jl","text":"julia> find_zero(f,  0.6) ≈ 0.550606579334135\ntrue","category":"page"},{"location":"","page":"Roots.jl","title":"Roots.jl","text":"This finds  the answer  to the left of the starting point. To get the other nearby zero, a starting point closer to the answer can be used.  However,  an initial graph might convince one  that any of the up-to-5 real  roots  will   occur between -5  and 5.  The find_zeros function uses  heuristics and a few of the  algorithms to  identify  all zeros between the specified range. Here  we see  there  are 3:","category":"page"},{"location":"","page":"Roots.jl","title":"Roots.jl","text":"julia> find_zeros(f, -5,  5)\n3-element Vector{Float64}:\n -1.0983313019186334\n  0.550606579334135\n  0.7690997031778959","category":"page"}]
}
